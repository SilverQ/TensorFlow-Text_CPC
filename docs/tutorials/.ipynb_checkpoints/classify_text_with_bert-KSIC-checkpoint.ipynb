{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb4espuLKJiA"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Hub Authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ6SNYq_tVVC"
   },
   "source": [
    "# Classify text to KSIC with BERT\n",
    "\n",
    "- Load the KSIC dataset (json format)\n",
    "- Load a BERT model from TensorFlow Hub\n",
    "- Build your own model by combining BERT with a classifier\n",
    "- Train your own model, fine-tuning BERT as part of that\n",
    "- Save your model and use it to classify sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnvd4mrtPHHV"
   },
   "source": [
    "### Read the KSIC dataset\n",
    "* json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pOdqCMoQDRJL"
   },
   "outputs": [],
   "source": [
    "# dataset_dir = os.path.join(os.getcwd(), 'patent')\n",
    "# train_dir = os.path.join(dataset_dir, 'train')\n",
    "f_list = ['ksic/ksic00.json', 'ksic/ksic01.json', 'ksic/ksic02.json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN9lWCYfPo7b"
   },
   "source": [
    "* 일반적인 json 포맷과는 차이가 있고, 매 line마다 dict 형식의 데이터가 하나씩 있는 형태\n",
    "* readline()으로 라인 단위로 읽어들인 후 json.loads(line)을 사용해 dict 형식으로 변환하여 dataframe에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6IwI_2bcIeX8"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 8\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_colnames(fn):\n",
    "    with open(fn, encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "#     print(line)\n",
    "    temp = json.loads(line)\n",
    "#     print(temp.keys())\n",
    "    return temp.keys()\n",
    "# 첫줄을 읽고 컬럼 구성을 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = read_colnames(f_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text 내의 \\\\\\\\\" 문자열로 인해 json 파싱 오류가 발생\n",
    "* escape 처리를 위해 replace 해 줌  :  line.replace('\\\\\\\\\"', '\\\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "error = []\n",
    "for fname in f_list[1:]:\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                temp.append(json.loads(line.replace('\\\\\\\\\"', '\\\\\"')))\n",
    "            except Exception as e:\n",
    "                error.append([e, line])\n",
    "# print(temp)\n",
    "raw_df = pd.DataFrame(data=temp, columns=col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n"
     ]
    }
   ],
   "source": [
    "print(len(error), error[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068453\n"
     ]
    }
   ],
   "source": [
    "# raw_df.head(1)\n",
    "print(len(raw_df))  # 1068453 확인 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Label dictionary\n",
    "* 전체 입력 데이터로부터 ksic의 중복 제거 리스트를 생성하고, index와 label간 변환을 위한 dict 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksic_label = raw_df['ksic'].unique()\n",
    "ksic_index_dict = {i:label for i, label in enumerate(ksic_label)}\n",
    "ksic_label_dict = {ksic_index_dict[key]:key for key in ksic_index_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28119    16022\n",
      "26121    15241\n",
      "27112    14819\n",
      "Name: ksic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "raw_df['ksic'].value_counts().to_csv('ksic/class_balance.csv')\n",
    "class_cocunt = raw_df['ksic'].value_counts()\n",
    "# 클래스 불균형이 존재하기 때문에 분류별 건수가 500건 이하인 경우는 제외하여 학습함\n",
    "print(class_cocunt[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class별 학습 데이터 수를 고려하여 500건 이상만 분류 예측\n",
    "* 500건 이하가 수집된 KSIC 분류는 관련 특허가 거의 없는 기술 분야로써, 분류 예측 학습에서 제외\n",
    "* 567개 KSIC 중 500건 미만인 KSIC는 제외하여 500개 label을 대상으로 학습 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 -> 500\n"
     ]
    }
   ],
   "source": [
    "class_cocunt2 = class_cocunt[class_cocunt>=500]\n",
    "print(len(class_cocunt), '->', len(class_cocunt2))\n",
    "# 567개 클래스 중 학습 데이터 건수가 500건 미만인 클래스는 학습에서 제외하여 500개 label을 대상으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28119    16022\n",
       "26121    15241\n",
       "27112    14819\n",
       "27199    14760\n",
       "27192    12979\n",
       "Name: ksic, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_cocunt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df2 = raw_df.loc[raw_df['ksic'].isin(class_cocunt2.keys())].copy()\n",
    "# A value is trying to be set on a copy of a slice from a DataFrame\n",
    "# 위와 같이 raw_df2가 raw_df의 복제본인데, raw_df2에서 데이터를 편집하면 원본과 불일치가 발생하게 된다.\n",
    "# 이 경우에는 .copy() 메서드를 사용하여 명시적으로 복사해서 사용하는게 좋다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068453 -> 1055662\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_df), '->', len(raw_df2))  # 1,068,453 건 중 학습 데이터가 500건 미만인 클래스를 제외하여 1,055,662건 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KSIC 코드를 정수 레이블로 변환\n",
    "raw_df2['label'] = raw_df2['ksic'].map(ksic_label_dict)\n",
    "# raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ksic</th>\n",
       "      <th>an</th>\n",
       "      <th>ad</th>\n",
       "      <th>pn</th>\n",
       "      <th>pd</th>\n",
       "      <th>rn</th>\n",
       "      <th>rd</th>\n",
       "      <th>ipc</th>\n",
       "      <th>cpc</th>\n",
       "      <th>title</th>\n",
       "      <th>ab</th>\n",
       "      <th>cl</th>\n",
       "      <th>apg</th>\n",
       "      <th>invt</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20411</td>\n",
       "      <td>1019570000830</td>\n",
       "      <td>19570723</td>\n",
       "      <td>1019580000974</td>\n",
       "      <td>19580228</td>\n",
       "      <td>100000313</td>\n",
       "      <td>19580702</td>\n",
       "      <td>C09D5/08</td>\n",
       "      <td>C09D5/084 | C09D193/00</td>\n",
       "      <td>방청 도료 제조법</td>\n",
       "      <td>국내에서 대량으로 구득할 수 있는 해조등으로 수입 품인 아마인유나 동유등을 절약할 ...</td>\n",
       "      <td>\\n</td>\n",
       "      <td>김동규</td>\n",
       "      <td>김동규</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ksic             an        ad             pn        pd         rn  \\\n",
       "0  20411  1019570000830  19570723  1019580000974  19580228  100000313   \n",
       "\n",
       "         rd       ipc                     cpc      title  \\\n",
       "0  19580702  C09D5/08  C09D5/084 | C09D193/00  방청 도료 제조법   \n",
       "\n",
       "                                                  ab  cl  apg invt  label  \n",
       "0  국내에서 대량으로 구득할 수 있는 해조등으로 수입 품인 아마인유나 동유등을 절약할 ...  \\n  김동규  김동규      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df2.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set과 test set 분할\n",
    "* 테스트셋의 비중은 20%, random_state=15, ksic를 기준으로 균등하게 20% 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input = train_test_split(raw_df2, random_state=15, test_size=0.2, stratify=raw_df2['ksic'], shuffle=True)\n",
    "train_input, val_input = train_test_split(train_input, random_state=15, test_size=0.15, stratify=train_input['ksic'], shuffle=True)\n",
    "# print(val_input.head(2))\n",
    "# print(train_input['ksic'].value_counts())\n",
    "# print(test_input['ksic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717849, 15) (126680, 15) (211133, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, val_input.shape, test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 저장\n",
    "train_input.to_csv('ksic/train_input.csv', encoding='utf-8', mode='w', index=False)\n",
    "val_input.to_csv('ksic/val_input.csv', encoding='utf-8', mode='w', index=False)\n",
    "test_input.to_csv('ksic/test_input.csv', encoding='utf-8', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트와 라벨쌍 추출\n",
    "* Title, Astract, Claim으로부터 텍스트를 추출하고, 'label' 컬럼과 데이터 쌍 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_text(df):\n",
    "    input_tl = df[['title', 'label']].copy()\n",
    "    input_tl.rename(columns={'title':'text'}, inplace=True)\n",
    "    input_ab = df[['ab', 'label']].copy()\n",
    "    input_ab.rename(columns={'ab':'text'}, inplace=True)\n",
    "    input_cl = df[['cl', 'label']].copy()\n",
    "    input_cl.rename(columns={'cl':'text'}, inplace=True)\n",
    "    input_text = pd.concat([input_tl, input_ab, input_cl]).copy()\n",
    "    input_text['text_len'] = input_text['text'].str.len()\n",
    "    input_text2 = input_text.loc[input_text['text_len'] > 3, ['text', 'label']].copy()  # 60813 rows × 3 columns 제거\n",
    "    return input_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 텍스트 데이터 추출\n",
    "# input_tl = raw_df[['title', 'label']]\n",
    "# input_tl.rename(columns={'title':'text'}, inplace=True)\n",
    "# input_ab = raw_df[['ab', 'label']]\n",
    "# input_ab.rename(columns={'ab':'text'}, inplace=True)\n",
    "# input_cl = raw_df[['cl', 'label']]\n",
    "# input_cl.rename(columns={'cl':'text'}, inplace=True)\n",
    "# # input_tl.head()\n",
    "# print(len(input_tl), len(input_ab), len(input_cl))\n",
    "# # 텍스트, 라벨쌍 합치기\n",
    "# input_text = pd.concat([input_tl, input_ab, input_cl])\n",
    "# input_text.head()\n",
    "# # 텍스트가 없거나 너무 짧은 데이터 제거\n",
    "# input_text['text_len'] = input_text['text'].str.len()\n",
    "# input_text = input_text.loc[input_text['text_len'] > 3, ['text', 'label']]  # 60813 rows × 3 columns 제거\n",
    "# print(len(input_text))  # 2979654 -> 2918120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text와 label쌍으로 이루어진 데이터 생성\n",
    "train_ds = make_input_text(train_input)\n",
    "val_ds = make_input_text(val_input)\n",
    "test_ds = make_input_text(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터와 테스트 데이터를 임시 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 저장\n",
    "train_ds.to_csv('ksic/train_ds.csv', encoding='utf-8', mode='w', index=False)\n",
    "val_ds.to_csv('ksic/val_ds.csv', encoding='utf-8', mode='w', index=False)\n",
    "test_ds.to_csv('ksic/test_ds.csv', encoding='utf-8', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "## Loading models from TensorFlow Hub\n",
    "\n",
    "Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
    "\n",
    "  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n",
    "  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
    "  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
    "  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
    "  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
    "  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n",
    "\n",
    "The model documentation on TensorFlow Hub has more details and references to the\n",
    "research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n",
    "printed after the next cell execution.\n",
    "\n",
    "The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\n",
    "one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n",
    "\n",
    "Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yield even better accuracy, but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
    "\n",
    "You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "y8_ctG55-uTX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "# bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "bert_model_name = 'bert_multi_cased_L-12_H-768_A-12'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(len(ksic_label), activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 16:34:25.145279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.194037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.194176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.195030: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-08 16:34:25.195483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.195586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.195673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.491775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.491905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.491993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 16:34:25.492068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9192 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_ds['text'], train_ds['label'])).shuffle(train_ds['text'].shape[0]).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_ds['text'], val_ds['label'])).shuffle(val_ds['text'].shape[0]).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_ds['text'], test_ds['label'])).shuffle(test_ds['text'].shape[0]).batch(batch_size)\n",
    "\n",
    "# tutorial의 dataset은 tf.keras.utils.text_dataset_from_directory를 사용하나,\n",
    "# 본 프로젝트에서는 paandas dataframe을 입력으로 하기 때문에 tf.data.Dataset.from_tensor_slices를 사용함\n",
    "# 아래 사이트를 참고하여 dataset의 형태를 맞춰줌, 다만 텍스트가 유니코드로 보여지는 문제가 존재함.\n",
    "# 학습에 문제가 없을지 점검 필요\n",
    "# https://stackoverflow.com/questions/69660201/inputting-some-data-for-bert-model-using-tf-data-dataset-from-tensor-slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.take(1)\n",
    "# <TakeDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
    "\n",
    "\n",
    "# tf.constant(train_dataset.take(1))\n",
    "# ValueError: Attempt to convert a value \n",
    "# (<TakeDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>)\n",
    "# with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>) to a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'\\xec\\xb2\\xa0\\xea\\xb0\\x95\\xec\\x9b\\x90\\xeb\\xa3\\x8c \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xeb\\xb0\\xa9\\xeb\\xb2\\x95\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c, \\xeb\\xb3\\x80\\xec\\x88\\x98\\xec\\x84\\xa0\\xec\\x96\\xb8 \\xeb\\xb0\\x8f \\xec\\xb4\\x88\\xea\\xb8\\xb0\\xec\\xb9\\x98 \\xec\\x84\\xa4\\xec\\xa0\\x95\\xeb\\x8b\\xa8\\xea\\xb3\\x84\\xec\\x99\\x80, \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xeb\\xa1\\x9c\\xeb\\xb6\\x80\\xed\\x84\\xb0 \\xec\\xb6\\x94\\xeb\\xa1\\xa0\\xec\\x9d\\x98\\xeb\\xa2\\xb0 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0\\xeb\\xa5\\xbc \\xeb\\x84\\xa4\\xed\\x8a\\xb8\\xec\\x9b\\x8d \\xec\\xa0\\x84\\xec\\x86\\xa1\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xec\\x88\\x98\\xec\\x8b\\xa0\\xeb\\xb0\\x9b\\xec\\x95\\x84\\xec\\x84\\x9c \\xec\\xb6\\x94\\xeb\\xa1\\xa0\\xea\\xb0\\x80\\xeb\\x8a\\xa5\\xed\\x95\\x9c \\xec\\x96\\xb8\\xec\\x96\\xb4\\xec\\x9d\\xb8 \\xec\\x98\\xa4\\xeb\\xb8\\x8c\\xec\\xa0\\x9d\\xed\\x8a\\xb8\\xed\\x98\\x95 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0\\xeb\\xa1\\x9c \\xeb\\x8b\\xa4\\xec\\x9d\\x8c\\xea\\xb3\\xbc \\xea\\xb0\\x99\\xec\\x9d\\xb4 \\xeb\\xb3\\x80\\xed\\x99\\x98\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xeb\\x8b\\xa8\\xea\\xb3\\x84\\xec\\x99\\x80,  \\xed\\x8c\\x8c\\xec\\x9d\\xbc \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xec\\xa7\\x80\\xec\\x8b\\x9c \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0 : \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c\\xeb\\xaa\\x85, \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c\\xeb\\xb3\\x84 \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xec\\xb4\\x9d\\xeb\\x9f\\x89, \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xea\\xb8\\xb0\\xea\\xb0\\x84, \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xed\\x8c\\xa8\\xed\\x84\\xb4(4\\xec\\xa2\\x85\\xec\\xa4\\x91 \\xed\\x83\\x9d\\xec\\x9d\\xbc) \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c\\xeb\\xb3\\x84 \\xec\\x84\\xb1\\xec\\x83\\x81 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0 : SiO2 \\xec\\x84\\xb1\\xeb\\xb6\\x84, \\xeb\\xb9\\x84\\xec\\xa4\\x91, \\xec\\x9e\\x85\\xeb\\x8f\\x84, \\xec\\x95\\x88\\xec\\x8b\\x9d\\xea\\xb0\\x81, \\xec\\xb2\\xa8\\xec\\xb0\\xa9\\xec\\x84\\xb1, \\xea\\xb4\\x91\\xec\\xa2\\x85 \\xeb\\xb0\\x8f \\xec\\x95\\xbc\\xeb\\x93\\x9c\\xec\\x9e\\xac\\xea\\xb3\\xa0 * \\xea\\xb4\\x91\\xec\\xa2\\x85(\\xed\\x8a\\xb9\\xec\\x88\\x98\\xea\\xb4\\x91, \\xeb\\xb6\\x80\\xec\\x9b\\x90\\xeb\\xa3\\x8c, \\xea\\xb5\\xad\\xeb\\x82\\xb4\\xea\\xb4\\x91, \\xec\\x88\\x98\\xec\\x9e\\x85\\xea\\xb4\\x91) \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xed\\x8c\\x8c\\xec\\x9d\\xbc\\xec\\xa0\\x81\\xeb\\xb6\\x80\\xec\\x8b\\xa4\\xec\\xa0\\x81 \\xeb\\x8d\\xb0\\xec\\x9d\\xb4\\xed\\x84\\xb0 : \\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x96\\xb4\\xeb\\xb3\\x84 \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xec\\x99\\x84\\xeb\\xa3\\x8c\\xeb\\x90\\x9c \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c\\xeb\\xaa\\x85 \\xeb\\xb0\\x8f \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xeb\\x9f\\x89, \\xec\\x8a\\xa4\\xed\\x83\\x9c\\xed\\x81\\xac \\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\x83\\x81\\xed\\x83\\x9c. \\xec\\xa7\\x80\\xec\\x8b\\x9d\\xeb\\xb2\\xa0\\xec\\x9d\\xb4\\xec\\x8a\\xa4\\xec\\x97\\x90 \\xed\\x8f\\xac\\xed\\x95\\xa8\\xeb\\x90\\x9c \\xea\\xb0\\x81\\xec\\xa2\\x85 \\xec\\x98\\xa4\\xeb\\xb8\\x8c\\xec\\xa0\\x9d\\xed\\x8a\\xb8, \\xec\\xbb\\xa8\\xec\\x8a\\xa4\\xed\\x8a\\xb8\\xeb\\xa0\\x88\\xec\\x9d\\xb8\\xed\\x8a\\xb8\\xeb\\x93\\xa4\\xec\\x9d\\x98 \\xed\\x94\\xbc\\xeb\\x9d\\xbc\\xeb\\xa9\\x94\\xed\\x84\\xb0 \\xed\\x85\\x8c\\xec\\x9d\\xb4\\xeb\\xb8\\x94\\xec\\x97\\x90 \\xec\\xb4\\x88\\xea\\xb8\\xb0\\xec\\xb9\\x98\\xeb\\xa5\\xbc \\xec\\x85\\x8b\\xed\\x8c\\x85\\xed\\x95\\x98\\xea\\xb3\\xa0 \\xec\\x8a\\xa4\\xec\\xbc\\x80\\xec\\xa4\\x84\\xeb\\xa7\\x81 \\xeb\\x8c\\x80\\xec\\x83\\x81 \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c\\xec\\x9d\\x98 \\xec\\xb6\\x94\\xeb\\xa1\\xa0 \\xec\\x9a\\xb0\\xec\\x84\\xa0\\xec\\x88\\x9c\\xec\\x9c\\x84\\xeb\\xa5\\xbc \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c\\xeb\\xb3\\x84 \\xec\\x84\\xb1\\xeb\\xb6\\x84\\xec\\x9d\\xb4 \\xeb\\xb6\\x88\\xec\\x95\\x88\\xec\\xa0\\x95\\xed\\x95\\x9c \\xec\\x88\\x9c\\xec\\x84\\x9c\\xeb\\xa1\\x9c \\xea\\xb2\\xb0\\xec\\xa0\\x95\\xed\\x95\\x9c \\xed\\x9b\\x84 \\xec\\x9a\\xb0\\xec\\x84\\xa0\\xec\\x88\\x9c\\xec\\x9c\\x84\\xea\\xb0\\x80 \\xeb\\x86\\x92\\xec\\x9d\\x80 \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c \\xec\\x88\\x9c\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x96\\xb4\\xec\\x97\\x90 \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xed\\x95\\xa0 \\xeb\\xb6\\x84\\xed\\x95\\xa0 \\xec\\x98\\xa4\\xeb\\x8d\\x94\\xec\\x88\\x98 \\xeb\\xb0\\x8f \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xeb\\x9f\\x89\\xec\\x9d\\x84 \\xea\\xb3\\x84\\xec\\x82\\xb0\\xed\\x95\\x98\\xec\\x97\\xac \\xec\\x98\\xa4\\xeb\\x8d\\x94 \\xeb\\xa6\\xac\\xec\\x8a\\xa4\\xed\\x8a\\xb8 \\xeb\\xb2\\x84\\xed\\x8d\\xbc\\xec\\x97\\x90 \\xec\\xa0\\x80\\xec\\x9e\\xa5\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xeb\\x8b\\xa8\\xea\\xb3\\x84\\xec\\x99\\x80, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\x8b\\xa8\\xea\\xb3\\x84\\xec\\x97\\x90\\xec\\x84\\x9c \\xeb\\xa7\\x8c\\xeb\\x93\\xa4\\xec\\x96\\xb4\\xec\\xa7\\x84 \\xec\\x98\\xa4\\xeb\\x8d\\x94 \\xeb\\xa6\\xac\\xec\\x8a\\xa4\\xed\\x8a\\xb8\\xec\\x97\\x90 \\xec\\xa0\\x80\\xec\\x9e\\xa5\\xeb\\x90\\x9c \\xec\\xb6\\x94\\xeb\\xa1\\xa0\\xec\\x9a\\xb0\\xec\\x84\\xa0 \\xec\\x88\\x9c\\xec\\x9c\\x84\\xeb\\xb3\\x84\\xeb\\xa1\\x9c \\xec\\xb6\\x94\\xeb\\xa1\\xa0\\xec\\x9d\\x84 \\xed\\x96\\x89\\xed\\x95\\x98\\xec\\x97\\xac \\xec\\xb5\\x9c\\xec\\xa2\\x85 \\xeb\\xaa\\xa9\\xec\\xa0\\x81\\xeb\\xb3\\x80\\xec\\x88\\x98\\xeb\\x9d\\xbc\\xea\\xb3\\xa0 \\xed\\x95\\xa0 \\xec\\x88\\x98 \\xec\\x9e\\x88\\xeb\\x8a\\x94 \\xec\\x98\\xa4\\xeb\\x8d\\x94\\xec\\x97\\x90 \\xeb\\x8c\\x80\\xed\\x95\\x9c \\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x96\\xb4\\xeb\\xb3\\x84 \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xec\\x9c\\x84\\xec\\xb9\\x98\\xeb\\xa5\\xbc \\xea\\xb2\\xb0\\xec\\xa0\\x95\\xed\\x95\\x98\\xea\\xb2\\x8c \\xeb\\x90\\x98\\xeb\\x8a\\x94\\xeb\\x8d\\xb0 \\xec\\xb6\\x94\\xeb\\xa1\\xa0\\xeb\\x8c\\x80\\xec\\x83\\x81 \\xec\\x98\\xa4\\xeb\\x8d\\x94\\xec\\x97\\x90 \\xeb\\x8c\\x80\\xed\\x95\\xb4 \\xec\\xbb\\xa8\\xec\\x8a\\xa4\\xed\\x8a\\xb8\\xeb\\xa0\\x88\\xec\\x9d\\xb8\\xed\\x8a\\xb8 \\xec\\xa0\\x9c\\xec\\x95\\xbd\\xed\\x95\\xa8\\xec\\x88\\x98 \\xec\\xb2\\xb4\\xed\\x81\\xac \\xeb\\xb0\\x8f \\xec\\xa1\\xb0\\xec\\xa0\\x95\\xec\\x9d\\x84 \\xec\\x9c\\x84\\xed\\x95\\x9c \\xed\\x8c\\x8c\\xec\\x8b\\x9c\\xeb\\xb9\\x8c\\xeb\\xa6\\xac\\xed\\x8b\\xb0 \\xec\\x96\\xb4\\xeb\\xa0\\x88\\xec\\x9d\\xb4 \\xeb\\xb2\\x84\\xed\\x8d\\xbc\\xeb\\xa5\\xbc \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x98 \\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x96\\xb4\\xec\\x88\\x98 \\xeb\\xa7\\x8c\\xed\\x81\\xbc \\xec\\x83\\x9d\\xec\\x84\\xb1\\xec\\x8b\\x9c\\xed\\x82\\xa4\\xea\\xb3\\xa0 \\xec\\x9d\\xb4 \\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x96\\xb4\\xeb\\xb3\\x84 \\xec\\x96\\xb4\\xeb\\xa0\\x88\\xec\\x9d\\xb4\\xec\\x97\\x90 \\xea\\xb0\\x80\\xec\\x9e\\xa5 \\xea\\xb0\\x80\\xec\\xa4\\x91\\xec\\xb9\\x98\\xea\\xb0\\x80 \\xed\\x95\\xa0\\xeb\\x8b\\xb9\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xed\\x94\\x8c\\xeb\\xa1\\x9c\\xec\\x96\\xb4\\xeb\\xa5\\xbc \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xec\\x9c\\x84\\xec\\xb9\\x98\\xeb\\xa1\\x9c \\xea\\xb2\\xb0\\xec\\xa0\\x95\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x98\\xa4\\xeb\\x8d\\x94\\xeb\\xb3\\x84 \\xec\\xa2\\x85\\xed\\x95\\xa9\\xec\\xb6\\x94\\xeb\\xa1\\xa0\\xeb\\x8b\\xa8\\xea\\xb3\\x84\\xeb\\xa5\\xbc \\xea\\xb5\\xac\\xeb\\xb9\\x84\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xea\\xb2\\x83\\xec\\x9d\\x84 \\xed\\x8a\\xb9\\xec\\xa7\\x95\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\xa0\\x84\\xeb\\xac\\xb8\\xea\\xb0\\x80 \\xec\\x8b\\x9c\\xec\\x8a\\xa4\\xed\\x85\\x9c\\xec\\x9d\\x84 \\xec\\x9d\\xb4\\xec\\x9a\\xa9\\xed\\x95\\x9c \\xec\\xb2\\xa0\\xea\\xb0\\x95\\xec\\x9b\\x90\\xeb\\xa3\\x8c \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xeb\\xb0\\xa9\\xeb\\xb2\\x95.\\\\n \\\\n \\xec\\xb2\\xa0\\xea\\xb0\\x95\\xec\\x9b\\x90\\xeb\\xa3\\x8c \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xec\\x9e\\xa5\\xec\\xb9\\x98\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c, \\xec\\xb2\\xa0\\xea\\xb0\\x95\\xec\\x9b\\x90\\xeb\\xa3\\x8c \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x84 \\xea\\xb2\\xb0\\xec\\xa0\\x95\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xec\\xa0\\x84\\xeb\\xac\\xb8\\xea\\xb0\\x80 \\xec\\x8b\\x9c\\xec\\x8a\\xa4\\xed\\x85\\x9c\\xea\\xb3\\xbc, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xec\\xa0\\x84\\xeb\\xac\\xb8\\xea\\xb0\\x80 \\xec\\x8b\\x9c\\xec\\x8a\\xa4\\xed\\x85\\x9c\\xec\\x97\\x90\\xec\\x84\\x9c \\xea\\xb2\\xb0\\xec\\xa0\\x95\\xeb\\x90\\x9c \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x97\\x90 \\xec\\x9d\\x98\\xed\\x95\\xb4 \\xec\\x84\\xa4\\xec\\xa0\\x95\\xea\\xb0\\x92\\xec\\x9d\\x84 \\xea\\xb3\\x84\\xec\\x82\\xb0\\xed\\x95\\x98\\xec\\x97\\xac \\xec\\x9e\\xa5\\xec\\xb9\\x98\\xeb\\xb3\\x84 \\xec\\x84\\xa4\\xeb\\xb9\\x84\\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\x83\\x81\\xed\\x83\\x9c\\xec\\xa0\\x95\\xeb\\xb3\\xb4\\xec\\x97\\x90 \\xeb\\x94\\xb0\\xeb\\x9d\\xbc \\xec\\x84\\xa4\\xeb\\xb9\\x84\\xec\\x9e\\x90\\xeb\\x8f\\x99\\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\xa0\\x9c\\xec\\x96\\xb4\\xed\\x95\\x98\\xea\\xb3\\xa0 \\xea\\xb7\\xa0\\xec\\xa7\\x88\\xed\\x99\\x94 \\xec\\xa0\\x81\\xeb\\xb6\\x80\\xec\\xa0\\x9c\\xec\\x96\\xb4 \\xeb\\xb0\\x8f \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xec\\x8b\\xa4\\xec\\xa0\\x81\\xec\\x9d\\x84 \\xea\\xb4\\x80\\xeb\\xa6\\xac\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xec\\x99\\x80, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xec\\x9d\\x98 \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xec\\x84\\xa4\\xeb\\xb9\\x84 \\xec\\xa2\\x85\\xed\\x95\\xa9\\xec\\x9a\\xb4\\xec\\xa0\\x84\\xed\\x99\\x94\\xeb\\xa9\\xb4 \\xeb\\xb0\\x8f \\xec\\x84\\xa4\\xeb\\xb9\\x84\\xec\\x9a\\xb4\\xec\\xa0\\x84 \\xec\\xa4\\x80\\xeb\\xb9\\x84\\xec\\x99\\x84\\xeb\\xa3\\x8c \\xec\\x83\\x81\\xed\\x83\\x9c\\xed\\x99\\x94\\xeb\\xa9\\xb4\\xec\\x9d\\x84 CRT\\xed\\x99\\x94\\xeb\\xa9\\xb4\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xed\\x91\\x9c\\xec\\x8b\\x9c\\xed\\x95\\x98\\xeb\\x8a\\x94 VDU\\xec\\x99\\x80, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xec\\x9d\\x98 \\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\xa7\\x80\\xeb\\xa0\\xb9 \\xec\\x8b\\xa0\\xed\\x98\\xb8\\xeb\\xa5\\xbc \\xec\\xa0\\x84\\xec\\x86\\xa1\\xeb\\xb0\\x9b\\xec\\x95\\x84 \\xeb\\x8b\\xa8\\xec\\x9c\\x84\\xec\\x9e\\xa5\\xec\\xb9\\x98\\xeb\\xb3\\x84\\xeb\\xa1\\x9c \\xea\\xb8\\xb0\\xeb\\x8f\\x99 \\xeb\\xb0\\x8f \\xec\\xa0\\x95\\xec\\xa7\\x80\\xec\\xb2\\x98\\xeb\\xa6\\xac\\xeb\\xa5\\xbc \\xed\\x96\\x89\\xed\\x95\\x98\\xea\\xb3\\xa0 \\xec\\x9e\\xa5\\xec\\xb9\\x98\\xeb\\xb3\\x84 \\xec\\x84\\xbc\\xec\\x84\\x9c\\xeb\\x93\\xa4\\xeb\\xa1\\x9c\\xeb\\xb6\\x80\\xed\\x84\\xb0 \\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\x83\\x81\\xed\\x83\\x9c\\xec\\x8b\\xa0\\xed\\x98\\xb8\\xeb\\xa5\\xbc \\xec\\x9e\\x85\\xeb\\xa0\\xa5\\xeb\\xb0\\x9b\\xec\\x95\\x84 \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xeb\\xa1\\x9c \\xec\\xa0\\x84\\xec\\x86\\xa1\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\x98\\xeb\\xa8\\xb8\\xeb\\xb8\\x94 \\xec\\xbd\\x98\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xec\\x99\\x80, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\x98\\xeb\\xa8\\xb8\\xeb\\xb8\\x94 \\xec\\xbd\\x98\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xec\\x9d\\x98 \\xec\\xa0\\x9c\\xec\\x96\\xb4\\xec\\x8b\\xa0\\xed\\x98\\xb8\\xec\\x9d\\xb8 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xec\\x97\\x90\\xec\\x84\\x9c \\xec\\x9e\\x90\\xeb\\x8f\\x99\\xea\\xb3\\x84\\xec\\x82\\xb0\\xeb\\x90\\x98\\xec\\x96\\xb4 \\xec\\xa0\\x84\\xec\\x86\\xa1\\xeb\\x90\\x98\\xec\\x96\\xb4\\xec\\xa7\\x84 \\xec\\x84\\xa4\\xec\\xa0\\x95\\xeb\\x9f\\x89\\xea\\xb3\\xbc \\xed\\x98\\xb8\\xed\\x8d\\xbc\\xeb\\xa1\\x9c\\xeb\\xb6\\x80\\xed\\x84\\xb0 \\xec\\xa0\\x88\\xec\\xb6\\x9c\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xeb\\xb8\\x8c\\xeb\\x9e\\x9c\\xeb\\x93\\x9c\\xec\\x9d\\x98 \\xec\\xa0\\x88\\xec\\xb6\\x9c\\xec\\xa4\\x91\\xeb\\x9f\\x89\\xec\\x9d\\x84 \\xea\\xb2\\x80\\xec\\xb6\\x9c\\xed\\x95\\x9c \\xed\\x98\\x84\\xec\\x9e\\xac \\xec\\x9e\\x91\\xec\\x97\\x85\\xec\\xa4\\x91\\xec\\x9d\\xb8 \\xec\\xa0\\x88\\xec\\xb6\\x9c\\xeb\\x9f\\x89\\xec\\x9d\\x84 \\xeb\\xb9\\x84\\xea\\xb5\\x90\\xed\\x95\\x98\\xec\\x97\\xac PID\\xec\\xa0\\x9c\\xec\\x96\\xb4\\xeb\\xb0\\x98\\xec\\x9d\\x84 \\xed\\x86\\xb5\\xed\\x95\\xb4 \\xec\\xa0\\x95\\xeb\\x9f\\x89\\xec\\xa0\\x88\\xec\\xb6\\x9c\\xec\\xa0\\x9c\\xec\\x96\\xb4\\xeb\\xa5\\xbc \\xed\\x96\\x89\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\xa0\\x95\\xeb\\x9f\\x89\\xec\\xa0\\x88\\xec\\xb6\\x9c\\xec\\xa0\\x9c\\xec\\x96\\xb4\\xeb\\xb0\\x98\\xea\\xb3\\xbc, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xec\\x9d\\x98 \\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\xa7\\x80\\xeb\\xa0\\xb9\\xec\\x8b\\xa0\\xed\\x98\\xb8\\xec\\x97\\x90 \\xeb\\x94\\xb0\\xeb\\x9d\\xbc \\xeb\\xb2\\xa8\\xed\\x8a\\xb8\\xec\\xbd\\x98\\xeb\\xb2\\xa0\\xec\\x96\\xb4\\xec\\x9d\\x98 \\xec\\x9a\\xb4\\xec\\xa0\\x84 \\xeb\\xb0\\x8f \\xec\\xa0\\x95\\xec\\xa7\\x80\\xec\\xa0\\x9c\\xec\\x96\\xb4\\xeb\\xa5\\xbc \\xed\\x96\\x89\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xeb\\xb2\\xa8\\xed\\x8a\\xb8\\xec\\xbd\\x98\\xeb\\xb2\\xa0\\xec\\x96\\xb4 \\xec\\xa0\\x9c\\xec\\x96\\xb4\\xeb\\xb0\\x98\\xea\\xb3\\xbc, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xed\\x94\\x84\\xeb\\xa1\\x9c\\xec\\x84\\xb8\\xec\\x8a\\xa4 \\xec\\xbb\\xb4\\xed\\x93\\xa8\\xed\\x84\\xb0\\xec\\x9d\\x98 \\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\xa7\\x80\\xeb\\xa0\\xb9\\xec\\x8b\\xa0\\xed\\x98\\xb8\\xeb\\xa5\\xbc \\xec\\x8b\\xa0\\xed\\x98\\xb8\\xeb\\xb3\\x80\\xed\\x99\\x98\\xea\\xb8\\xb0\\xec\\x99\\x80 \\xec\\x9c\\xa0\\xeb\\x8f\\x84\\xeb\\xac\\xb4\\xec\\x84\\xa0 \\xec\\xbc\\x80\\xec\\x9d\\xb4\\xeb\\xb8\\x94\\xec\\x9d\\x84 \\xed\\x86\\xb5\\xed\\x95\\x98\\xec\\x97\\xac \\xeb\\xac\\xb4\\xec\\x84\\xa0\\xec\\x95\\x88\\xed\\x85\\x8c\\xeb\\x82\\x98\\xeb\\xa1\\x9c \\xeb\\xb0\\x9b\\xec\\x95\\x84 \\xec\\x8a\\xa4\\xed\\x83\\x9c\\xed\\x81\\xac \\xec\\xa3\\xbc\\xed\\x96\\x89 \\xeb\\xaa\\xa8\\xed\\x83\\x80\\xec\\x9d\\x98 \\xec\\x9a\\xb4\\xec\\xa0\\x84 \\xeb\\xb0\\x8f \\xec\\xa0\\x95\\xec\\xa7\\x80\\xec\\xa0\\x9c\\xec\\x96\\xb4\\xeb\\xa5\\xbc \\xed\\x96\\x89\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x8a\\xa4\\xed\\x83\\x9c\\xed\\x81\\xac \\xec\\x9a\\xb4\\xec\\xa0\\x84\\xec\\xa0\\x9c\\xec\\x96\\xb4\\xeb\\xb0\\x98\\xec\\x9d\\x84 \\xea\\xb5\\xac\\xeb\\xb9\\x84\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xea\\xb2\\x83\\xec\\x9d\\x84 \\xed\\x8a\\xb9\\xec\\xa7\\x95\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\xa0\\x84\\xeb\\xac\\xb8\\xea\\xb0\\x80 \\xec\\x8b\\x9c\\xec\\x8a\\xa4\\xed\\x85\\x9c\\xec\\x9d\\x84 \\xec\\x9d\\xb4\\xec\\x9a\\xa9\\xed\\x95\\x9c \\xec\\xb2\\xa0\\xea\\xb0\\x95\\xec\\x9b\\x90\\xeb\\xa3\\x8c \\xeb\\xb0\\xb0\\xed\\x95\\xa9\\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n \\\\n'\n",
      "Label : 11 (24111)\n",
      "Review: b'\\xeb\\xa1\\x9c\\xec\\x9a\\xb0\\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x84 \\xed\\x98\\x95\\xec\\x84\\xb1\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90 \\xeb\\xaa\\xa8\\xeb\\x93\\x88;\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xa1\\x9c\\xec\\x9a\\xb0\\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xea\\xb3\\xbc \\xec\\xa4\\x91\\xec\\xb2\\xa9\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xec\\xa0\\x9c1 \\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x84 \\xed\\x98\\x95\\xec\\x84\\xb1\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xeb\\xb3\\xb5\\xec\\x88\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xea\\xb3\\xbc, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x98 \\xec\\x83\\x81\\xec\\xb8\\xa1\\xec\\x97\\x90 \\xeb\\xb0\\xb0\\xec\\xb9\\x98\\xeb\\x90\\x98\\xeb\\xa9\\xb0, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c1 \\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xeb\\xb3\\xb4\\xeb\\x8b\\xa4 \\xec\\x9b\\x90\\xea\\xb1\\xb0\\xeb\\xa6\\xac\\xec\\x9d\\x98 \\xec\\xa0\\x9c2 \\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x84 \\xed\\x98\\x95\\xec\\x84\\xb1\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xeb\\xb3\\xb5\\xec\\x88\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c2 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x84 \\xed\\x8f\\xac\\xed\\x95\\xa8\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\xa0\\x9c2 \\xea\\xb4\\x91\\xec\\x9b\\x90 \\xeb\\xaa\\xa8\\xeb\\x93\\x88; \\xeb\\xb0\\x8f\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90 \\xeb\\xb0\\x8f \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c2 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x84 \\xea\\xb0\\x9c\\xeb\\xb3\\x84 \\xec\\xa0\\x9c\\xec\\x96\\xb4\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\xbb\\xa8\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xeb\\xa5\\xbc \\xed\\x8f\\xac\\xed\\x95\\xa8\\xed\\x95\\x98\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n\\\\t\\\\t \\xec\\xa0\\x9c1\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xbb\\xa8\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xeb\\x8a\\x94, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xea\\xb3\\xbc \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c2 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x84 \\xec\\x98\\xa8/\\xec\\x98\\xa4\\xed\\x94\\x84(ON/OFF)\\xed\\x95\\x98\\xea\\xb1\\xb0\\xeb\\x82\\x98 \\xea\\xb4\\x91\\xeb\\x8f\\x84\\xeb\\xa5\\xbc \\xec\\xa1\\xb0\\xec\\xa0\\x88\\xed\\x95\\x98\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n\\\\t\\\\t \\xec\\xa0\\x9c1\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xbb\\xa8\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9d\\xb4 \\xea\\xb3\\xa1\\xec\\x84\\xa0\\xeb\\xa1\\x9c\\xeb\\xa5\\xbc \\xec\\xa3\\xbc\\xed\\x96\\x89\\xec\\x8b\\x9c, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xb3\\xb5\\xec\\x88\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90 \\xec\\xa4\\x91 \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xea\\xb3\\xa1\\xec\\x84\\xa0\\xeb\\xa1\\x9c\\xec\\x97\\x90 \\xeb\\x8c\\x80\\xec\\x9d\\x91\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xeb\\xb0\\xa9\\xed\\x96\\xa5\\xec\\x97\\x90 \\xeb\\xb0\\xb0\\xec\\xb9\\x98\\xeb\\x90\\x9c \\xec\\xa0\\x81\\xec\\x96\\xb4\\xeb\\x8f\\x84 \\xed\\x95\\x98\\xeb\\x82\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x84 \\xec\\x98\\xa8(ON)\\xed\\x95\\x98\\xea\\xb1\\xb0\\xeb\\x82\\x98 \\xea\\xb4\\x91\\xeb\\x8f\\x84\\xeb\\xa5\\xbc \\xec\\xa6\\x9d\\xea\\xb0\\x80\\xec\\x8b\\x9c\\xed\\x82\\xa4\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n\\\\t\\\\t \\xec\\xa0\\x9c3\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xbb\\xa8\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xec\\x97\\x90 \\xec\\x9d\\x98\\xed\\x95\\xb4 \\xec\\x98\\xa8(ON)\\xeb\\x90\\x98\\xea\\xb1\\xb0\\xeb\\x82\\x98 \\xea\\xb4\\x91\\xeb\\x8f\\x84\\xea\\xb0\\x80 \\xec\\xa6\\x9d\\xea\\xb0\\x80\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x98 \\xea\\xb0\\x9c\\xec\\x88\\x98\\xeb\\x8a\\x94, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xea\\xb3\\xa1\\xec\\x84\\xa0\\xeb\\xa1\\x9c\\xec\\x9d\\x98 \\xea\\xb3\\xa1\\xeb\\xa5\\xa0\\xec\\x97\\x90 \\xeb\\xb9\\x84\\xeb\\xa1\\x80\\xed\\x95\\x98\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98. \\\\n\\\\t\\\\t \\xec\\xa0\\x9c1\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xbb\\xa8\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xeb\\x8a\\x94, \\xeb\\x8c\\x80\\xed\\x96\\xa5 \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9d\\x98 \\xec\\x9c\\x84\\xec\\xb9\\x98\\xec\\x97\\x90 \\xeb\\x8c\\x80\\xec\\x9d\\x91\\xeb\\x90\\x98\\xeb\\x8a\\x94 \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c2 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x84 \\xec\\x98\\xa4\\xed\\x94\\x84(OFF)\\xec\\x8b\\x9c\\xed\\x82\\xa4\\xea\\xb1\\xb0\\xeb\\x82\\x98 \\xea\\xb4\\x91\\xeb\\x8f\\x84\\xeb\\xa5\\xbc \\xea\\xb0\\x90\\xec\\x86\\x8c\\xec\\x8b\\x9c\\xed\\x82\\xa4\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n\\\\t\\\\t \\xec\\xa0\\x9c1\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xb3\\xb5\\xec\\x88\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xea\\xb3\\xbc \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xb3\\xb5\\xec\\x88\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c2 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x80 \\xed\\x9a\\xa1 \\xeb\\xb0\\xa9\\xed\\x96\\xa5\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xeb\\xb0\\xb0\\xec\\x97\\xb4\\xeb\\x90\\x98\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98. \\\\n\\\\t\\\\t \\xec\\xa0\\x9c1\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xa1\\x9c\\xec\\x9a\\xb0\\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x97\\x90\\xeb\\x8a\\x94, \\xec\\xbb\\xb7-\\xec\\x98\\xa4\\xed\\x94\\x84 \\xeb\\x9d\\xbc\\xec\\x9d\\xb8\\xec\\x9d\\xb4 \\xed\\x98\\x95\\xec\\x84\\xb1\\xeb\\x90\\x98\\xea\\xb3\\xa0,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c1 \\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x80, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xbb\\xb7-\\xec\\x98\\xa4\\xed\\x94\\x84 \\xeb\\x9d\\xbc\\xec\\x9d\\xb8\\xec\\x9d\\x98 \\xed\\x95\\x98\\xec\\xb8\\xa1\\xec\\x97\\x90 \\xed\\x98\\x95\\xec\\x84\\xb1\\xeb\\x90\\x98\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n\\\\t\\\\t \\xec\\xa0\\x9c1\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xbb\\xa8\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xeb\\x8a\\x94, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xb3\\xb5\\xec\\x88\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c2 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x84 \\xec\\x98\\xa8(ON)\\xed\\x95\\x98\\xea\\xb1\\xb0\\xeb\\x82\\x98 \\xea\\xb4\\x91\\xeb\\x8f\\x84\\xeb\\xa5\\xbc \\xec\\xa6\\x9d\\xea\\xb0\\x80\\xec\\x8b\\x9c\\xec\\xbc\\x9c \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c2 \\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\xb4 \\xed\\x95\\x98\\xec\\x9d\\xb4 \\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x84 \\xed\\x98\\x95\\xec\\x84\\xb1\\xed\\x95\\x98\\xeb\\x8f\\x84\\xeb\\xa1\\x9d \\xed\\x95\\x98\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n\\\\t\\\\t \\xec\\xa0\\x9c1\\xed\\x95\\xad\\xec\\x97\\x90 \\xec\\x9e\\x88\\xec\\x96\\xb4\\xec\\x84\\x9c,\\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xbb\\xa8\\xed\\x8a\\xb8\\xeb\\xa1\\xa4\\xeb\\x9f\\xac\\xeb\\x8a\\x94, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xb3\\xb5\\xec\\x88\\x98\\xec\\x9d\\x98 \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90 \\xec\\xa4\\x91 \\xec\\xa4\\x91\\xea\\xb0\\x84 \\xec\\x98\\x81\\xec\\x97\\xad\\xec\\x97\\x90 \\xeb\\xb0\\xb0\\xec\\xb9\\x98\\xeb\\x90\\x9c \\xec\\xa0\\x9c1 \\xea\\xb4\\x91\\xec\\x9b\\x90\\xec\\x9d\\x84 \\xec\\x98\\xa8(ON)\\xed\\x95\\x98\\xea\\xb1\\xb0\\xeb\\x82\\x98 \\xea\\xb4\\x91\\xeb\\x8f\\x84\\xeb\\xa5\\xbc \\xec\\xa6\\x9d\\xea\\xb0\\x80\\xec\\x8b\\x9c\\xec\\xbc\\x9c \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xec\\xa0\\x9c1 \\xeb\\xb9\\x94 \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\xb4 \\xec\\x8a\\xa4\\xed\\x8c\\x9f \\xed\\x8c\\xa8\\xed\\x84\\xb4\\xec\\x9d\\x84 \\xed\\x98\\x95\\xec\\x84\\xb1\\xed\\x95\\x98\\xeb\\x8f\\x84\\xeb\\xa1\\x9d \\xed\\x95\\x98\\xeb\\x8a\\x94, \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\x9e\\xa8\\xed\\x94\\x84 \\xec\\x9e\\xa5\\xec\\xb9\\x98.\\\\n\\\\t\\\\t'\n",
      "Label : 212 (28421)\n",
      "Review: b'\\xeb\\xb3\\xb8 \\xea\\xb3\\xa0\\xec\\x95\\x88\\xec\\x9d\\x80 \\xeb\\xb6\\x88\\xec\\x88\\x9c\\xeb\\xac\\xbc\\xec\\x9d\\x84 \\xeb\\x82\\xb4\\xed\\x8f\\xac\\xed\\x95\\x98\\xea\\xb3\\xa0 \\xec\\x9e\\x88\\xeb\\x8a\\x94 \\xec\\x98\\xa4\\xec\\x9d\\xbc\\xec\\x9d\\xb4 \\xec\\x97\\x94\\xec\\xa7\\x84\\xec\\x9d\\x98 rpm \\xec\\xa6\\x9d\\xea\\xb0\\x80\\xec\\x97\\x90\\xeb\\x8f\\x84 \\xeb\\xb6\\x88\\xea\\xb5\\xac\\xed\\x95\\x98\\xea\\xb3\\xa0 \\xec\\x97\\xac\\xea\\xb3\\xbc\\xea\\xb0\\x80 \\xec\\x9d\\xb4\\xeb\\xa3\\xa8\\xec\\x96\\xb4\\xec\\xa7\\x88 \\xec\\x88\\x98 \\xec\\x9e\\x88\\xeb\\x8a\\x94 \\xea\\xb5\\xac\\xec\\xa1\\xb0\\xeb\\xa5\\xbc \\xec\\x8b\\xa4\\xed\\x98\\x84\\xed\\x95\\xa8\\xec\\x9c\\xbc\\xeb\\xa1\\x9c\\xec\\x8d\\xa8 \\xec\\x8b\\xa0\\xeb\\xa2\\xb0\\xeb\\xa5\\xbc \\xed\\x9a\\x8c\\xeb\\xb3\\xb5\\xed\\x95\\xa0 \\xec\\x88\\x98 \\xec\\x9e\\x88\\xea\\xb2\\x8c \\xed\\x95\\x98\\xea\\xb3\\xa0, \\xeb\\x98\\x90\\xed\\x95\\x9c \\xec\\xa0\\x95\\xed\\x99\\x94\\xeb\\x90\\x9c \\xec\\x98\\xa4\\xec\\x9d\\xbc\\xec\\x9d\\xb4 \\xed\\x95\\xad\\xec\\x83\\x81 \\xec\\x97\\x94\\xec\\xa7\\x84\\xec\\x9d\\x98 \\xea\\xb0\\x81 \\xeb\\xb6\\x80\\xeb\\xb6\\x84\\xec\\x9c\\xbc\\xeb\\xa1\\x9c \\xea\\xb3\\xb5\\xea\\xb8\\x89\\xeb\\x90\\x98\\xea\\xb2\\x8c \\xed\\x95\\xa8\\xec\\x9c\\xbc\\xeb\\xa1\\x9c\\xec\\x8d\\xa8 \\xec\\x97\\x94\\xec\\xa7\\x84\\xec\\x84\\xb1\\xeb\\x8a\\xa5 \\xed\\x96\\xa5\\xec\\x83\\x81\\xea\\xb3\\xbc \\xed\\x95\\xa8\\xea\\xbb\\x98 \\xec\\x88\\x98\\xeb\\xaa\\x85\\xec\\x9d\\xb4 \\xec\\x97\\xb0\\xec\\x9e\\xa5\\xeb\\x90\\xa0 \\xec\\x88\\x98 \\xec\\x9e\\x88\\xeb\\x8f\\x84\\xeb\\xa1\\x9d \\xed\\x95\\x98\\xeb\\x8a\\x94\\xeb\\x8d\\xb0\\xec\\x97\\x90 \\xec\\x9e\\x88\\xeb\\x8a\\x94 \\xea\\xb2\\x83\\xec\\x9c\\xbc\\xeb\\xa1\\x9c\\xec\\x84\\x9c, \\xea\\xb7\\xb8 \\xea\\xb5\\xac\\xec\\xa1\\xb0\\xeb\\x8a\\x94 \\xec\\x97\\x94\\xec\\xa7\\x84\\xec\\x9d\\x84 \\xea\\xb5\\xac\\xec\\x84\\xb1\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x8b\\xa4\\xeb\\xa6\\xb0\\xeb\\x8d\\x94 \\xeb\\xb8\\x94\\xeb\\xa1\\x9d \\xec\\x9d\\xbc\\xec\\xb8\\xa1\\xec\\x97\\x90, \\xec\\x98\\xa4\\xec\\x9d\\xbc\\xec\\x9d\\xb4 \\xec\\x9c\\xa0\\xec\\x9e\\x85\\xeb\\x90\\xa0 \\xec\\x88\\x98 \\xec\\x9e\\x88\\xea\\xb2\\x8c \\xec\\x95\\x88\\xeb\\x82\\xb4\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xec\\x9d\\xb8\\xeb\\xa0\\x9b \\xed\\x99\\x80\\xea\\xb3\\xbc \\xec\\x98\\xa4\\xec\\x9d\\xbc\\xec\\x95\\x95\\xeb\\xa0\\xa5\\xec\\x9d\\x84 \\xec\\xa0\\x81\\xec\\xa0\\x95\\xed\\x95\\x98\\xea\\xb2\\x8c \\xec\\x9c\\xa0\\xec\\xa7\\x80\\xec\\x8b\\x9c\\xed\\x82\\xa4\\xeb\\x8a\\x94 \\xeb\\xb0\\x94\\xec\\x9d\\xb4\\xed\\x8c\\xa8\\xec\\x8a\\xa4 \\xeb\\xb0\\xb8\\xeb\\xb8\\x8c\\xea\\xb0\\x80 \\xea\\xb5\\xac\\xeb\\xb9\\x84\\xeb\\x90\\x9c \\xeb\\xa0\\x88\\xec\\x8a\\xa4\\xed\\x8a\\xb8 \\xeb\\xa3\\xb8\\xec\\x9d\\xb4 \\xed\\x98\\x95\\xec\\x84\\xb1\\xeb\\x90\\x98\\xea\\xb3\\xa0, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xa0\\x88\\xec\\x8a\\xa4\\xed\\x8a\\xb8 \\xeb\\xa3\\xb8\\xec\\x97\\x90 \\xec\\x98\\xa4\\xec\\x9d\\xbc\\xed\\x95\\x84\\xed\\x84\\xb0\\xeb\\xa5\\xbc \\xed\\x86\\xb5\\xea\\xb3\\xbc\\xed\\x95\\x9c \\xec\\x98\\xa4\\xec\\x9d\\xbc\\xec\\x9d\\x98 \\xed\\x9d\\x90\\xeb\\xa6\\x84\\xec\\x9d\\x84 \\xec\\x95\\x88\\xeb\\x82\\xb4\\xed\\x95\\x98\\xeb\\x8a\\x94 \\xed\\x99\\x80\\xec\\x9d\\xb4 \\xec\\xa4\\x91\\xec\\x95\\x99\\xec\\x97\\x90 \\xec\\x9c\\x84\\xec\\xb9\\x98\\xed\\x95\\x9c \\xeb\\xb3\\xb4\\xec\\x8a\\xa4\\xec\\x97\\x90 \\xed\\x98\\x95\\xec\\x84\\xb1\\xeb\\x90\\x98\\xeb\\xa9\\xb0, \\xec\\x9d\\xb4\\xec\\x99\\x80 \\xed\\x95\\xa8\\xea\\xbb\\x98 \\xec\\x95\\x84\\xec\\x9b\\x83\\xeb\\xa0\\x9b \\xed\\x99\\x80\\xec\\x9d\\xb4 \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xed\\x99\\x80\\xea\\xb3\\xbc \\xeb\\xb0\\x94\\xec\\x9d\\xb4\\xed\\x8c\\xa8\\xec\\x8a\\xa4 \\xeb\\xb0\\xb8\\xeb\\xb8\\x8c\\xec\\x97\\x90 \\xec\\x97\\xb0\\xea\\xb2\\xb0\\xeb\\x90\\x98\\xeb\\xa9\\xb4\\xec\\x84\\x9c \\xec\\x8b\\xa4\\xeb\\xa6\\xb0\\xeb\\x8d\\x94 \\xeb\\xb8\\x94\\xeb\\xa1\\x9d\\xec\\x97\\x90 \\xed\\x98\\x95\\xec\\x84\\xb1\\xeb\\x90\\x98\\xeb\\x8a\\x94\\xeb\\x8d\\xb0, \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xeb\\xb3\\xb4\\xec\\x8a\\xa4\\xec\\x9d\\x98 \\xed\\x99\\x80\\xec\\x97\\x90 \\xea\\xb3\\xa0\\xec\\xa0\\x95\\xed\\x8a\\x9c\\xeb\\xb8\\x8c\\xea\\xb0\\x80 \\xec\\x82\\xbd, \\xed\\x83\\x88 \\xea\\xb0\\x80\\xeb\\x8a\\xa5\\xed\\x95\\x98\\xea\\xb2\\x8c \\xec\\x84\\xa4\\xec\\xb9\\x98\\xeb\\x90\\x98\\xea\\xb3\\xa0, \\xec\\x9d\\xb4 \\xea\\xb3\\xa0\\xec\\xa0\\x95\\xed\\x8a\\x9c\\xeb\\xb8\\x8c \\xec\\x83\\x81\\xeb\\xb6\\x80\\xec\\x97\\x90 \\xea\\xb0\\x80\\xec\\x9d\\xb4\\xeb\\x93\\x9c \\xed\\x8a\\x9c\\xeb\\xb8\\x8c\\xea\\xb0\\x80 \\xed\\x9a\\x8c\\xeb\\x8f\\x99 \\xea\\xb0\\x80\\xeb\\x8a\\xa5\\xed\\x95\\x98\\xea\\xb2\\x8c \\xea\\xb2\\xb0\\xed\\x95\\xa9\\xeb\\x90\\x98\\xeb\\xa9\\xb4\\xec\\x84\\x9c \\xec\\x95\\x84\\xec\\x9b\\x83\\xeb\\xa0\\x9b \\xed\\x99\\x80\\xec\\x97\\x90 \\xec\\x9c\\x84\\xec\\xb9\\x98\\xed\\x95\\x98\\xeb\\x90\\x98, \\xec\\x98\\xa4\\xec\\x9d\\xbc\\xec\\x97\\x90 \\xeb\\x82\\xb4\\xed\\x8f\\xac\\xeb\\x90\\x9c \\xeb\\xb6\\x88\\xec\\x88\\x9c\\xeb\\xac\\xbc\\xec\\x9d\\x84 \\xec\\x97\\xac\\xea\\xb3\\xbc\\xed\\x95\\x98\\xea\\xb8\\xb0 \\xec\\x9c\\x84\\xed\\x95\\x9c \\xed\\x95\\x84\\xed\\x84\\xb0\\xeb\\xa5\\xbc \\xec\\x83\\x81\\xea\\xb8\\xb0 \\xea\\xb0\\x80\\xec\\x9d\\xb4\\xeb\\x93\\x9c \\xed\\x8a\\x9c\\xeb\\xb8\\x8c\\xea\\xb0\\x80 \\xea\\xb5\\xac\\xeb\\xb9\\x84\\xed\\x95\\x98\\xeb\\xa9\\xb0 \\xec\\x84\\xa4\\xec\\xb9\\x98\\xeb\\x90\\x9c \\xec\\xb0\\xa8\\xeb\\x9f\\x89\\xec\\x9a\\xa9 \\xeb\\xb3\\xb4\\xec\\xa1\\xb0 \\xec\\x98\\xa4\\xec\\x9d\\xbc \\xec\\x97\\xac\\xea\\xb3\\xbc\\xea\\xb8\\xb0\\xeb\\xa5\\xbc \\xec\\xa0\\x9c\\xea\\xb3\\xb5 \\xed\\x95\\x98\\xeb\\x8a\\x94\\xeb\\x8d\\xb0\\xec\\x97\\x90 \\xec\\x9e\\x88\\xeb\\x8b\\xa4. \\\\n'\n",
      "Label : 65 (29175)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_dataset.take(1):\n",
    "    for i in range(3):\n",
    "        print(f'Review: {text_batch.numpy()[i]}')\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f'Label : {label} ({ksic_index_dict[label]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_raw_result = classifier_model(tf.constant(train_dataset.take(1)))\n",
    "# print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# help(tf.data.Dataset.from_tensor_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.keras.utils.text_dataset_from_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfds.load(np.array(input_text))\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_ds['text'], train_ds['label']))\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((val_ds['text'], val_ds['label']))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_ds['text'], test_ds['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "init_lr = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
      "Epoch 1/3\n",
      "     4/263508 [..............................] - ETA: 3:37:56 - loss: 6.2632 - sparse_categorical_crossentropy: 12.2350 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 16:35:50.545744: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263508/263508 [==============================] - 15111s 57ms/step - loss: 5.5095 - sparse_categorical_crossentropy: 10.2013 - val_loss: 5.2838 - val_sparse_categorical_crossentropy: 8.9203\n",
      "Epoch 2/3\n",
      "133009/263508 [==============>...............] - ETA: 1:47:06 - loss: 5.2429 - sparse_categorical_crossentropy: 8.4173"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_dataset, validation_data=val_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method call in module keras.engine.functional:\n",
      "\n",
      "call(inputs, training=None, mask=None) method of keras.engine.functional.Functional instance\n",
      "    Calls the model on new inputs.\n",
      "    \n",
      "    In this case `call` just reapplies\n",
      "    all ops in the graph to the new inputs\n",
      "    (e.g. build a new computational graph from the provided inputs).\n",
      "    \n",
      "    Args:\n",
      "        inputs: A tensor or list of tensors.\n",
      "        training: Boolean or boolean scalar tensor, indicating whether to run\n",
      "          the `Network` in training mode or inference mode.\n",
      "        mask: A mask or list of masks. A mask can be\n",
      "            either a tensor or None (no mask).\n",
      "    \n",
      "    Returns:\n",
      "        A tensor if there is a single output, or\n",
      "        a list of tensors if there are more than one outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help(classifier_model.compute_metrics)\n",
    "help(classifier_model.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77511/77511 [==============================] - 3840s 50ms/step - loss: 5.1026 - sparse_categorical_crossentropy: 7.4670\n",
      "Loss: 5.10257625579834\n",
      "Accuracy: 7.466965675354004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 10:33:20.131510: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_dataset)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "dataset_name = 'patent'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)\n",
    "\n",
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## Export for inference\n",
    "\n",
    "Now you just save your fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ShcvqJAgVera"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'patent'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbI25bS1vD7s"
   },
   "source": [
    "Let's reload the model, so you can try it side by side with the model that is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyTappHTvNCz"
   },
   "source": [
    "Here you can test your model on any sentence you want, just add to the examples variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "    result_for_printing = \\\n",
    "        [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "                                                 for i in range(len(inputs))]\n",
    "    print(*result_for_printing, sep='\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    '본 발명은 OMA BCAST 서비스에서, 다양한 포맷으로 제작된 콘테츠들에 대해 각 가입자 단말기에서 별도의 변환 프로세싱 없이 콘텐츠를 재생할 수 있도록 서비스 제공자 측에서 각 가입자의 단말기에 적합하게 트랜스코딩하여 전송하는 방법에 관한 것이다.',  # this is the same sentence tried earlier\n",
    "    '마스터 패킷 제어기와 데이터 서비스 노드간의 Ｒ-Ｐ 연결실패시 브로드캐스트/멀티캐스트 서비스 제공 방법 및시스템',\n",
    "    '모듈식 MBMS(MULTIMEDIA BROADCAST AND MULTICAST SERVICE) 전달을 위한 기술들',\n",
    "    '통신 단말기를 이용한 방송관련 채팅 서비스 방법 및시스템',\n",
    "    '상황 기반 비정상 모니터링을 위한 방법들 및 시스템들',\n",
    "    '사용자의 프로파일 정보를 이용한 광고 시청을 통한 무료통화 서비스 제공 시스템 및 그 방법'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(np.random.randint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118141 105327 204745 152614  30150 197861  15173 157631 103258 188773]\n"
     ]
    }
   ],
   "source": [
    "sample_index = np.random.randint(len(test_input), size=10)\n",
    "print(sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = raw_df2[raw_df2.iloc[sample_index], ['ksic','title','ab','cl','label']]\n",
    "samples = test_input.iloc[sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        연번                        항목명\n",
      "코드                                   \n",
      "01110    1           곡물 및 기타 식량작물 재배업\n",
      "01121    2                   채소작물 재배업\n",
      "01122    3                   화훼작물 재배업\n",
      "01123    4                종자 및 묘목 생산업\n",
      "01131    5                   과실작물 재배업\n",
      "...    ...                        ...\n",
      "62090  563  기타 정보 기술 및 컴퓨터 운영 관련 서비스업\n",
      "ICT출판  564         디지털출판 콘텐츠 개발 및 제작업\n",
      "ICT영상  565         디지털영상 콘텐츠 개발 및 제작업\n",
      "ICT음악  566         디지털음악 콘텐츠 개발 및 제작업\n",
      "ICT교육  567         디지털교육 콘텐츠 개발 및 제작업\n",
      "\n",
      "[567 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "ksic_label = pd.read_csv('ksic/KSIC_567_label.csv', sep='\\t')\n",
    "ksic_label = ksic_label.set_index('코드')\n",
    "print(ksic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'화훼작물 재배업'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksic_label.loc['01122']['항목명']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요약문을 사용한 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['28122' '20202' '28202' '29272' '27112' '28410' '21230' '33992' '25922'\n",
      " '10797'] \n",
      " ['본 고안은 가입자의 댁내에 설치되어 광 가입자 망을 구축 형성하고자 사용되고 있는 광 네트워크 단말장치에 관한 것이다. \\\\n 본 고안에 따른 광 네트워크 단말장치에 의하면, 외부로부터 연결되는 광 커넥터가 케이스의 내부에 전부 삽입되는 상태로 접속되도록 하여, 상기 광 커넥터가 케이스의 외부로 노출되지 않는 구조이므로, 사용자의 실수 혹은 호기심에 따른 광 커넥터의 파손을 방지하고, 특히 어린이가 호기심에 의해 광 커넥터를 분리하여 광원(레이저 빛)을 눈에 비출 경우, 시력이 손상되거나 혹은 상실되는 것을 방지할 수 있도록 한 것이다.\\\\n 나아가, 본 고안에서는 외부로부터 연결되는 광 커넥터를 접속 및 분리할 경우, 특수 제작된 별도의 집게를 사용하도록 하여, 임의적인 분리로 인한 광 커넥터의 손상시, 가입자와 설치업체간에 책임소지를 투명하게 함으로써, 합리적인 관리가 이루어질 수 있도록 한 것이다.\\\\n 본 고안은, 광 송수신기로부터 인출된 광 커넥터와 접속하게 되는 광 아답터가 접속공과 일치하는 상태로 수평 배치되면서 접속공을 형성한 케이스의 내측면으로부터 거리를 두고 설치되어, 외부로부터 접속공을 통해 광 아답터와 접속시, 광 커넥터가 외부로 노출되지 않도록 케이스 내부로 전부 수용됨으로써, 광 커넥터의 노출에 따른 파손과 손상이 방지되도록 하는 광 네트워크 단말장치가 제공된다.  \\\\n 또한, 본 고안은, 외부로부터 연결되는 광 커넥터가 케이스 내부로 전부 수용되는 상태로 광 송수신기에 접속될 수 있도록 하기 위하여, 상기 광 송수신기가 접속공을 형성한 케이스의 내측면으로부터 거리를 두고 설치됨으로써, 상기 케이스 외부로 광 커넥터의 노출에 따른 파손과 손상이 방지되도록 하는 광 네트워크 단말장치가 제공된다.  \\\\n'\n",
      " '본 발명은 반도체소자 밀봉용 고기능 에폭시수지의 제조방법에 관한 것으로서, 하기 구조식(Ⅱ)의 에폭시수지를 비극성 유기용매에 녹이고 소량의 벤조일피록사이드 촉매를 가하여 교반하면서 1-브로모-2, 5-피롤리딘디완(C H Br NO)을 적하하여 하기 구조식(Ⅲ)의 브롬이 치환된 에폭시수지를 수득하고, 이것을 디에틸에테르 또는 테트라하이드로푸란에 마그네슘과 함께 녹이고 O-메틸하이드록실아민을 넣고 교반하여 이민그릅이 치환된 하기 구조식(Ⅳ)의 에폭시수지를 수득한 후, 하기 구조식(Ⅴ)의 말레이미드와 함께 DMF에 녹여 수시간 반응시켜 수득하는 것을 특징으로 한 일반식(Ⅰ)의 반도체소자 밀봉용 고기능 에폭시수지의 제조방법을 제공하는 것이다.\\\\n \\\\n \\\\n \\\\n \\\\n \\\\n (여기에서 R1, R2는 H 또는 (CH2)nCH3기이고, n은 0 또는 1 이상의 정수이다.)\\\\n']\n"
     ]
    }
   ],
   "source": [
    "samples.head(1)\n",
    "examples = samples['ab'].to_numpy()\n",
    "examples_label = samples['ksic'].to_numpy()\n",
    "print(examples_label, '\\n', examples[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79  89 547  49 561 547   6 561  49  18]\n",
      "KSIC: 28122(전기회로 접속장치 제조업)\n",
      "예측: 28119(기타 전기 변환장치 제조업)\n",
      "Abst: 본 고안은 가입자의 댁내에 설치되어 광 가입자 망을 구축 형성하고자 사용되고 있는 광 네트\n",
      "\n",
      "KSIC: 20202(합성수지 및 기타 플라스틱 물질 제조업)\n",
      "예측: 20202(합성수지 및 기타 플라스틱 물질 제조업)\n",
      "Abst: 본 발명은 반도체소자 밀봉용 고기능 에폭시수지의 제조방법에 관한 것으로서, 하기 구조식(Ⅱ\n",
      "\n",
      "KSIC: 28202(축전지 제조업)\n",
      "예측: 26121(발광 다이오드 제조업)\n",
      "Abst: \\n\\t\\t본 발명은 이차 전지 전극용 바인더 조성물 및 이를 포함하는 전극 합제에 관한 \n",
      "\n",
      "KSIC: 29272(디스플레이 제조용 기계 제조업)\n",
      "예측: 26521(라디오, 녹음 및 재생 기기 제조업)\n",
      "Abst: 내용 없음.\\n\n",
      "\n",
      "KSIC: 27112(전기식 진단 및 요법 기기 제조업)\n",
      "예측: 27112(전기식 진단 및 요법 기기 제조업)\n",
      "Abst: 더모코스메틱, 의료 또는 심미적 치료들을 위한 레이저 디바이스는,\\nA) 램프-펌프 소스를\n",
      "\n",
      "KSIC: 28410(전구 및 램프 제조업)\n",
      "예측: 26121(발광 다이오드 제조업)\n",
      "Abst: 본 발명의 LED모듈을 이용한 조립식 등기구에 관한 것으로, 부채꼴의 원호상으로 다수 형성\n",
      "\n",
      "KSIC: 21230(동물용 의약품 제조업)\n",
      "예측: 21210(완제 의약품 제조업)\n",
      "Abst: 본 발명은 글루탐산염 이상을 치료하기 위한 방법 및 NAALADase 저해제를 사용하여 동\n",
      "\n",
      "KSIC: 33992(라이터, 연소물 및 흡연용품 제조업)\n",
      "예측: 27112(전기식 진단 및 요법 기기 제조업)\n",
      "Abst: 액체 기화 흡입 장치가 개시된다.\\n 개시되는 액체 기화 흡입 장치는 사용자가 흡입할 수 \n",
      "\n",
      "KSIC: 25922(도금업)\n",
      "예측: 26521(라디오, 녹음 및 재생 기기 제조업)\n",
      "Abst: 내용 없음.\\n\n",
      "\n",
      "KSIC: 10797(건강 기능식품 제조업)\n",
      "예측: 27192(정형 외과용 및 신체 보정용 기기 제조업)\n",
      "Abst: 일측면에서 본 발명은 특정 효소가 처리된 홍삼추출물 및 인삼 열매 추출물을 일정 비율로 포\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = classifier_model.predict(tf.constant(examples))\n",
    "result_predicted = np.argmax(result, axis = 1)\n",
    "print(result_predicted)\n",
    "for i, pred in enumerate(result_predicted):\n",
    "    print('KSIC: %s(%s)\\n예측: %s(%s)\\nAbst: %s\\n'%(examples_label[i], ksic_label.loc[examples_label[i]]['항목명'], ksic_index_dict[pred], ksic_label.loc[ksic_index_dict[pred]]['항목명'], examples[i][:50]))\n",
    "#     print('KSIC: %s, predict: %s, Abst: %s\\n'%(examples_label[i], ksic_index_dict[pred], examples[i][:50]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 발명의 명칭을 사용한 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSIC: 28122(전기회로 접속장치 제조업)\n",
      "예측: 26521(라디오, 녹음 및 재생 기기 제조업)\n",
      "Abst: 광 네트워크 단말장치\n",
      "\n",
      "KSIC: 20202(합성수지 및 기타 플라스틱 물질 제조업)\n",
      "예측: 20202(합성수지 및 기타 플라스틱 물질 제조업)\n",
      "Abst: 반도체소자 밀봉용 고기능 에폭시수지의 제조방법\n",
      "\n",
      "KSIC: 28202(축전지 제조업)\n",
      "예측: 28202(축전지 제조업)\n",
      "Abst: 이차 전지 전극용 바인더 조성물 및 전극 합제\n",
      "\n",
      "KSIC: 29272(디스플레이 제조용 기계 제조업)\n",
      "예측: 29132(기체 펌프 및 압축기 제조업)\n",
      "Abst: 액정표시소자 제조용 롤코팅기\n",
      "\n",
      "KSIC: 27112(전기식 진단 및 요법 기기 제조업)\n",
      "예측: 27112(전기식 진단 및 요법 기기 제조업)\n",
      "Abst: 더모코스메틱 치료들을 위한 레이저 디바이스 및 추적 키트\n",
      "\n",
      "KSIC: 28410(전구 및 램프 제조업)\n",
      "예측: 29132(기체 펌프 및 압축기 제조업)\n",
      "Abst: ＬＥＤ모듈을 이용한 조립식 등기구\n",
      "\n",
      "KSIC: 21230(동물용 의약품 제조업)\n",
      "예측: 21210(완제 의약품 제조업)\n",
      "Abst: 날라다아제조성물, 글루탐산염 이상 치료법 및 동물내의 뉴우런활성법\n",
      "\n",
      "KSIC: 33992(라이터, 연소물 및 흡연용품 제조업)\n",
      "예측: 29132(기체 펌프 및 압축기 제조업)\n",
      "Abst: 액체 기화 흡입 장치\n",
      "\n",
      "KSIC: 25922(도금업)\n",
      "예측: 26291(전자 축전기 제조업)\n",
      "Abst: 도금(鍍金) 처리용 바렐\n",
      "\n",
      "KSIC: 10797(건강 기능식품 제조업)\n",
      "예측: 20423(화장품 제조업)\n",
      "Abst: 홍삼추출물 및 인삼 열매 추출물을 포함하는 조성물\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples.head(1)\n",
    "examples = samples['title'].to_numpy()\n",
    "examples_label = samples['ksic'].to_numpy()\n",
    "# print(examples_label, '\\n', examples[:2])\n",
    "result = classifier_model.predict(tf.constant(examples))\n",
    "result_predicted = np.argmax(result, axis = 1)\n",
    "# print(result_predicted)\n",
    "for i, pred in enumerate(result_predicted):\n",
    "    print('KSIC: %s(%s)\\n예측: %s(%s)\\nAbst: %s\\n'%(examples_label[i], ksic_label.loc[examples_label[i]]['항목명'], ksic_index_dict[pred], ksic_label.loc[ksic_index_dict[pred]]['항목명'], examples[i][:50]))\n",
    "#     print('KSIC: %s, predict: %s, Abst: %s\\n'%(examples_label[i], ksic_index_dict[pred], examples[i][:50]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set을 사용한 예측\n",
    "* 발명의 명칭, 요약, 청구항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSIC: 38230(건설 폐기물 처리업)\n",
      "예측: 28903(교통 신호장치 제조업)\n",
      "Abst: 건설폐기물로부터 시멘트 페이스트 및 모르타르가 제거된재생골재 및 모래를 생산하는 방법 및 \n",
      "\n",
      "KSIC: 25942(그 외 금속 파스너 및 나사제품 제조업)\n",
      "예측: 28519(기타 가정용 전기 기기 제조업)\n",
      "Abst: 이탈 방지 장치를 갖는 나사\n",
      "\n",
      "KSIC: 29271(반도체 제조용 기계 제조업)\n",
      "예측: 27192(정형 외과용 및 신체 보정용 기기 제조업)\n",
      "Abst: 반도체 제조용 열교환기 누수 보수보강방법\n",
      "\n",
      "KSIC: 27302(사진기, 영사기 및 관련 장비 제조업)\n",
      "예측: 27302(사진기, 영사기 및 관련 장비 제조업)\n",
      "Abst: 카메라용 손떨림 보정장치\n",
      "\n",
      "KSIC: 27302(사진기, 영사기 및 관련 장비 제조업)\n",
      "예측: 27302(사진기, 영사기 및 관련 장비 제조업)\n",
      "Abst: 카메라의 파인더 렌즈 배율 가변 시스템\n",
      "\n",
      "KSIC: 19101(코크스 및 관련제품 제조업)\n",
      "예측: 27112(전기식 진단 및 요법 기기 제조업)\n",
      "Abst: 원료 처리 장치 및 그 처리 방법\n",
      "\n",
      "KSIC: 62021(컴퓨터 시스템 통합 자문 및 구축 서비스업)\n",
      "예측: 27112(전기식 진단 및 요법 기기 제조업)\n",
      "Abst: 주문형 맞춤 설계를 위한 데이터베이스 공유형 제품 설계시스템 및 그 방법\n",
      "\n",
      "KSIC: 29163(컨베이어 장치 제조업)\n",
      "예측: 29172(공기 조화장치 제조업)\n",
      "Abst: 수평 이송 장치\n",
      "\n",
      "KSIC: 26529(기타 음향기기 제조업)\n",
      "예측: 26129(기타 반도체 소자 제조업)\n",
      "Abst: 전자 장치 및 전자 장치의 오디오 처리 방법\n",
      "\n",
      "KSIC: 24290(기타 1차 비철금속 제조업)\n",
      "예측: 28119(기타 전기 변환장치 제조업)\n",
      "Abst: 연주설비의 디버러 제어 방법\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = test_input['title'].to_numpy()\n",
    "examples_label = test_input['ksic'].to_numpy()\n",
    "result = classifier_model.predict(tf.constant(examples))\n",
    "result_predicted = np.argmax(result, axis = 1)\n",
    "for i, pred in enumerate(result_predicted[:10]):\n",
    "    print('KSIC: %s(%s)\\n예측: %s(%s)\\nAbst: %s\\n'%(examples_label[i], ksic_label.loc[examples_label[i]]['항목명'], ksic_index_dict[pred], ksic_label.loc[ksic_index_dict[pred]]['항목명'], examples[i][:50]))\n",
    "#     print('KSIC: %s, predict: %s, Abst: %s\\n'%(examples_label[i], ksic_index_dict[pred], examples[i][:50]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211133,)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = test_input.copy()\n",
    "test_result.shape  # (211133, 15)\n",
    "result_predicted.shape  # (211133,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result['pred_tl'] = result_predicted\n",
    "test_result['pred_tl_ksic'] = test_result['pred'].map(ksic_index_dict)\n",
    "# test_result.to_csv('ksic/test_result.csv')\n",
    "# ksic\tan\tad\tpn\tpd\trn\trd\tipc\tcpc\ttitle\tab\tcl\tapg\tinvt\tlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ksic</th>\n",
       "      <th>an</th>\n",
       "      <th>ad</th>\n",
       "      <th>pn</th>\n",
       "      <th>pd</th>\n",
       "      <th>rn</th>\n",
       "      <th>rd</th>\n",
       "      <th>ipc</th>\n",
       "      <th>cpc</th>\n",
       "      <th>title</th>\n",
       "      <th>ab</th>\n",
       "      <th>cl</th>\n",
       "      <th>apg</th>\n",
       "      <th>invt</th>\n",
       "      <th>label</th>\n",
       "      <th>ab2</th>\n",
       "      <th>cl2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ksic, an, ad, pn, pd, rn, rd, ipc, cpc, title, ab, cl, apg, invt, label, ab2, cl2]\n",
       "Index: []"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input[test_input['ab'].isna()==True]\n",
    "test_input['ab2'] = test_input['ab'].fillna(test_input['title'])\n",
    "test_input[test_input['ab2'].isna()==True]\n",
    "\n",
    "test_input[test_input['ab'].isna()==True]\n",
    "test_input['cl2'] = test_input['cl'].fillna(test_input['ab2'])\n",
    "test_input[test_input['cl2'].isna()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = test_input['ab2'].to_numpy()\n",
    "ab_result = classifier_model.predict(tf.constant(examples))\n",
    "ab_predicted = np.argmax(ab_result, axis = 1)\n",
    "test_result['pred_ab'] = ab_predicted\n",
    "test_result['pred_ab_ksic'] = test_result['pred_ab'].map(ksic_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "examples = test_input['cl2'].to_numpy()\n",
    "cl_result = classifier_model.predict(tf.constant(examples))\n",
    "cl_predicted = np.argmax(cl_result, axis = 1)\n",
    "test_result['pred_cl'] = cl_predicted\n",
    "test_result['pred_cl_ksic'] = test_result['pred_cl'].map(ksic_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('ksic/test_result.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20411': 0,\n",
       " '25121': 1,\n",
       " '20491': 2,\n",
       " '30110': 3,\n",
       " '11111': 4,\n",
       " '23231': 5,\n",
       " '21210': 6,\n",
       " '29210': 7,\n",
       " '19102': 8,\n",
       " '14192': 9,\n",
       " '29193': 10,\n",
       " '24111': 11,\n",
       " '20132': 12,\n",
       " '20493': 13,\n",
       " '15211': 14,\n",
       " '29180': 15,\n",
       " '20311': 16,\n",
       " '27216': 17,\n",
       " '27192': 18,\n",
       " '38312': 19,\n",
       " '13401': 20,\n",
       " '27199': 21,\n",
       " '42491': 22,\n",
       " '10401': 23,\n",
       " '24211': 24,\n",
       " '17110': 25,\n",
       " '33920': 26,\n",
       " '10220': 27,\n",
       " '20321': 28,\n",
       " '10802': 29,\n",
       " '10794': 30,\n",
       " '20111': 31,\n",
       " '22199': 32,\n",
       " '29242': 33,\n",
       " '23232': 34,\n",
       " '28111': 35,\n",
       " '13101': 36,\n",
       " '29150': 37,\n",
       " '29194': 38,\n",
       " '23311': 39,\n",
       " '18121': 40,\n",
       " '10742': 41,\n",
       " '24113': 42,\n",
       " '12000': 43,\n",
       " '10713': 44,\n",
       " '19221': 45,\n",
       " '13211': 46,\n",
       " '24131': 47,\n",
       " '33992': 48,\n",
       " '26521': 49,\n",
       " '20499': 50,\n",
       " '29191': 51,\n",
       " '27302': 52,\n",
       " '19229': 53,\n",
       " '15219': 54,\n",
       " '24219': 55,\n",
       " '25119': 56,\n",
       " '16102': 57,\n",
       " '10792': 58,\n",
       " '24213': 59,\n",
       " '10743': 60,\n",
       " '27219': 61,\n",
       " '35200': 62,\n",
       " '20494': 63,\n",
       " '10403': 64,\n",
       " '29175': 65,\n",
       " '20121': 66,\n",
       " '28121': 67,\n",
       " '10799': 68,\n",
       " '28902': 69,\n",
       " '23119': 70,\n",
       " '29291': 71,\n",
       " '29141': 72,\n",
       " '25922': 73,\n",
       " '24191': 74,\n",
       " '10212': 75,\n",
       " '33991': 76,\n",
       " '24212': 77,\n",
       " '21220': 78,\n",
       " '28119': 79,\n",
       " '20112': 80,\n",
       " '33303': 81,\n",
       " '25923': 82,\n",
       " '33209': 83,\n",
       " '20129': 84,\n",
       " '10712': 85,\n",
       " '29293': 86,\n",
       " '20501': 87,\n",
       " '33110': 88,\n",
       " '20202': 89,\n",
       " '10751': 90,\n",
       " '29299': 91,\n",
       " '33932': 92,\n",
       " '10730': 93,\n",
       " '29176': 94,\n",
       " '19210': 95,\n",
       " '22229': 96,\n",
       " '10611': 97,\n",
       " '23211': 98,\n",
       " '15110': 99,\n",
       " '35112': 100,\n",
       " '11201': 101,\n",
       " '28201': 102,\n",
       " '10795': 103,\n",
       " '10309': 104,\n",
       " '25995': 105,\n",
       " '21300': 106,\n",
       " '23325': 107,\n",
       " '27400': 108,\n",
       " '22214': 109,\n",
       " '41223': 110,\n",
       " '29230': 111,\n",
       " '10502': 112,\n",
       " '22291': 113,\n",
       " '25931': 114,\n",
       " '10711': 115,\n",
       " '10801': 116,\n",
       " '20131': 117,\n",
       " '01123': 118,\n",
       " '33201': 119,\n",
       " '20423': 120,\n",
       " '22193': 121,\n",
       " '10211': 122,\n",
       " '31113': 123,\n",
       " '23111': 124,\n",
       " '10612': 125,\n",
       " '16211': 126,\n",
       " '20413': 127,\n",
       " '41222': 128,\n",
       " '35120': 129,\n",
       " '10129': 130,\n",
       " '29223': 131,\n",
       " '16103': 132,\n",
       " '29241': 133,\n",
       " '21230': 134,\n",
       " '03211': 135,\n",
       " '24290': 136,\n",
       " '23994': 137,\n",
       " '10501': 138,\n",
       " '29131': 139,\n",
       " '41224': 140,\n",
       " '10793': 141,\n",
       " '24222': 142,\n",
       " '10301': 143,\n",
       " '16212': 144,\n",
       " '27194': 145,\n",
       " '24112': 146,\n",
       " '29174': 147,\n",
       " '29172': 148,\n",
       " '29192': 149,\n",
       " '10402': 150,\n",
       " '42135': 151,\n",
       " '29294': 152,\n",
       " '22221': 153,\n",
       " '35113': 154,\n",
       " '01140': 155,\n",
       " '23919': 156,\n",
       " '22211': 157,\n",
       " '23999': 158,\n",
       " '26410': 159,\n",
       " '25942': 160,\n",
       " '22299': 161,\n",
       " '25130': 162,\n",
       " '26600': 163,\n",
       " '29111': 164,\n",
       " '24119': 165,\n",
       " '01151': 166,\n",
       " '20422': 167,\n",
       " '20492': 168,\n",
       " '29292': 169,\n",
       " '25914': 170,\n",
       " '28112': 171,\n",
       " '24221': 172,\n",
       " '28512': 173,\n",
       " '33401': 174,\n",
       " '22212': 175,\n",
       " '29261': 176,\n",
       " '01121': 177,\n",
       " '29269': 178,\n",
       " '22192': 179,\n",
       " '33910': 180,\n",
       " '28903': 181,\n",
       " '26294': 182,\n",
       " '26112': 183,\n",
       " '26221': 184,\n",
       " '23121': 185,\n",
       " '28423': 186,\n",
       " '33301': 187,\n",
       " '29163': 188,\n",
       " '35300': 189,\n",
       " '28909': 190,\n",
       " '25992': 191,\n",
       " '20201': 192,\n",
       " '26292': 193,\n",
       " '24132': 194,\n",
       " '22232': 195,\n",
       " '38220': 196,\n",
       " '20119': 197,\n",
       " '23222': 198,\n",
       " '28410': 199,\n",
       " '08000': 200,\n",
       " '30392': 201,\n",
       " '41111': 202,\n",
       " '42201': 203,\n",
       " '26111': 204,\n",
       " '10796': 205,\n",
       " '29250': 206,\n",
       " '01220': 207,\n",
       " '10213': 208,\n",
       " '23191': 209,\n",
       " '10122': 210,\n",
       " '10121': 211,\n",
       " '28421': 212,\n",
       " '29221': 213,\n",
       " '23129': 214,\n",
       " '33933': 215,\n",
       " '23324': 216,\n",
       " '28302': 217,\n",
       " '27191': 218,\n",
       " '28901': 219,\n",
       " '29162': 220,\n",
       " '02011': 221,\n",
       " '25911': 222,\n",
       " '27309': 223,\n",
       " '30320': 224,\n",
       " '25934': 225,\n",
       " '42203': 226,\n",
       " '21102': 227,\n",
       " '14111': 228,\n",
       " '26291': 229,\n",
       " '07210': 230,\n",
       " '42136': 231,\n",
       " '22223': 232,\n",
       " '25991': 233,\n",
       " '33931': 234,\n",
       " '29169': 235,\n",
       " '26421': 236,\n",
       " '29199': 237,\n",
       " '14419': 238,\n",
       " '42204': 239,\n",
       " '27214': 240,\n",
       " '24121': 241,\n",
       " '59201': 242,\n",
       " '23993': 243,\n",
       " '24312': 244,\n",
       " '28303': 245,\n",
       " '24322': 246,\n",
       " '24122': 247,\n",
       " '01110': 248,\n",
       " '23192': 249,\n",
       " '10613': 250,\n",
       " '38210': 251,\n",
       " '10219': 252,\n",
       " '24123': 253,\n",
       " '33409': 254,\n",
       " '29173': 255,\n",
       " '25912': 256,\n",
       " '32091': 257,\n",
       " '27213': 258,\n",
       " '29142': 259,\n",
       " '20203': 260,\n",
       " '25929': 261,\n",
       " '25943': 262,\n",
       " '22111': 263,\n",
       " '42123': 264,\n",
       " '01231': 265,\n",
       " '26295': 266,\n",
       " '24311': 267,\n",
       " '30122': 268,\n",
       " '42500': 269,\n",
       " '10620': 270,\n",
       " '33309': 271,\n",
       " '14199': 272,\n",
       " '30310': 273,\n",
       " '31202': 274,\n",
       " '29224': 275,\n",
       " '25932': 276,\n",
       " '31991': 277,\n",
       " '01299': 278,\n",
       " '25993': 279,\n",
       " '22292': 280,\n",
       " '32019': 281,\n",
       " '10749': 282,\n",
       " '41210': 283,\n",
       " '10720': 284,\n",
       " '35114': 285,\n",
       " '10791': 286,\n",
       " '31114': 287,\n",
       " '14200': 288,\n",
       " '23212': 289,\n",
       " '35111': 290,\n",
       " '33993': 291,\n",
       " '03112': 292,\n",
       " '19101': 293,\n",
       " '13910': 294,\n",
       " '39009': 295,\n",
       " '10741': 296,\n",
       " '28122': 297,\n",
       " '30201': 298,\n",
       " '26429': 299,\n",
       " '30331': 300,\n",
       " '01131': 301,\n",
       " '14120': 302,\n",
       " '26224': 303,\n",
       " '25924': 304,\n",
       " '24321': 305,\n",
       " '31111': 306,\n",
       " '30203': 307,\n",
       " '31311': 308,\n",
       " '29280': 309,\n",
       " '29119': 310,\n",
       " '25200': 311,\n",
       " '23312': 312,\n",
       " '21101': 313,\n",
       " '24199': 314,\n",
       " '29132': 315,\n",
       " '33202': 316,\n",
       " '22249': 317,\n",
       " '25944': 318,\n",
       " '10111': 319,\n",
       " '32029': 320,\n",
       " '23992': 321,\n",
       " '20421': 322,\n",
       " '29133': 323,\n",
       " '14112': 324,\n",
       " '15220': 325,\n",
       " '22241': 326,\n",
       " '28123': 327,\n",
       " '25123': 328,\n",
       " '23323': 329,\n",
       " '28113': 330,\n",
       " '29120': 331,\n",
       " '13221': 332,\n",
       " '35119': 333,\n",
       " '31322': 334,\n",
       " '32011': 335,\n",
       " '31201': 336,\n",
       " '20424': 337,\n",
       " '30391': 338,\n",
       " '22231': 339,\n",
       " '27193': 340,\n",
       " '23112': 341,\n",
       " '38240': 342,\n",
       " '27111': 343,\n",
       " '42137': 344,\n",
       " '13300': 345,\n",
       " '28429': 346,\n",
       " '22112': 347,\n",
       " '17211': 348,\n",
       " '23221': 349,\n",
       " '30121': 350,\n",
       " '42134': 351,\n",
       " '22222': 352,\n",
       " '42311': 353,\n",
       " '42420': 354,\n",
       " '59111': 355,\n",
       " '22191': 356,\n",
       " '29161': 357,\n",
       " '25122': 358,\n",
       " '60100': 359,\n",
       " '41225': 360,\n",
       " '22259': 361,\n",
       " '20502': 362,\n",
       " '41122': 363,\n",
       " '31920': 364,\n",
       " '23122': 365,\n",
       " '26321': 366,\n",
       " '25112': 367,\n",
       " '30332': 368,\n",
       " '30393': 369,\n",
       " '42138': 370,\n",
       " '42411': 371,\n",
       " '01291': 372,\n",
       " '42121': 373,\n",
       " '42131': 374,\n",
       " '10619': 375,\n",
       " '29229': 376,\n",
       " '16300': 377,\n",
       " '10759': 378,\n",
       " '31999': 379,\n",
       " '42110': 380,\n",
       " '25941': 381,\n",
       " '01211': 382,\n",
       " '14194': 383,\n",
       " '03213': 384,\n",
       " '30399': 385,\n",
       " '25999': 386,\n",
       " '23991': 387,\n",
       " '26222': 388,\n",
       " '42122': 389,\n",
       " '14193': 390,\n",
       " '28301': 391,\n",
       " '26293': 392,\n",
       " '38321': 393,\n",
       " '23995': 394,\n",
       " '25994': 395,\n",
       " '20495': 396,\n",
       " '42129': 397,\n",
       " '42412': 398,\n",
       " '26310': 399,\n",
       " '41221': 400,\n",
       " '58219': 401,\n",
       " '33402': 402,\n",
       " '31120': 403,\n",
       " '28422': 404,\n",
       " '61210': 405,\n",
       " '18111': 406,\n",
       " '15129': 407,\n",
       " '14411': 408,\n",
       " '42132': 409,\n",
       " '03120': 410,\n",
       " '32099': 411,\n",
       " '10797': 412,\n",
       " '31910': 413,\n",
       " '31112': 414,\n",
       " '42139': 415,\n",
       " '14499': 416,\n",
       " '38230': 417,\n",
       " '62021': 418,\n",
       " '60222': 419,\n",
       " '02020': 420,\n",
       " '22251': 421,\n",
       " '25933': 422,\n",
       " '29171': 423,\n",
       " '38322': 424,\n",
       " '33302': 425,\n",
       " '16299': 426,\n",
       " '41129': 427,\n",
       " '10302': 428,\n",
       " '23199': 429,\n",
       " '01411': 430,\n",
       " '25913': 431,\n",
       " '03220': 432,\n",
       " '26223': 433,\n",
       " '58113': 434,\n",
       " '23911': 435,\n",
       " '22213': 436,\n",
       " '16291': 437,\n",
       " '25111': 438,\n",
       " '42312': 439,\n",
       " '01122': 440,\n",
       " '59120': 441,\n",
       " '01152': 442,\n",
       " '16232': 443,\n",
       " '03111': 444,\n",
       " '27211': 445,\n",
       " '37022': 446,\n",
       " '31321': 447,\n",
       " '02012': 448,\n",
       " '20412': 449,\n",
       " '16101': 450,\n",
       " '26219': 451,\n",
       " '33120': 452,\n",
       " '58190': 453,\n",
       " '41119': 454,\n",
       " '37021': 455,\n",
       " '36020': 456,\n",
       " '42202': 457,\n",
       " '31312': 458,\n",
       " '15121': 459,\n",
       " '01412': 460,\n",
       " '62022': 461,\n",
       " '01239': 462,\n",
       " '38110': 463,\n",
       " '16292': 464,\n",
       " '26322': 465,\n",
       " '62090': 466,\n",
       " '42322': 467,\n",
       " '41226': 468,\n",
       " '03212': 469,\n",
       " '25113': 470,\n",
       " '60229': 471,\n",
       " '58212': 472,\n",
       " '42499': 473,\n",
       " '15190': 474,\n",
       " '23321': 475,\n",
       " '16221': 476,\n",
       " '59141': 477,\n",
       " '01132': 478,\n",
       " '29272': 479,\n",
       " '61220': 480,\n",
       " '23229': 481,\n",
       " '23322': 482,\n",
       " '01300': 483,\n",
       " '42321': 484,\n",
       " '42133': 485,\n",
       " '58221': 486,\n",
       " '42492': 487,\n",
       " '39001': 488,\n",
       " '01212': 489,\n",
       " '23239': 490,\n",
       " '28114': 491,\n",
       " '38120': 492,\n",
       " '58121': 493,\n",
       " '59112': 494,\n",
       " '10112': 495,\n",
       " '59114': 496,\n",
       " '42209': 497,\n",
       " '41229': 498,\n",
       " '38311': 499,\n",
       " '36010': 500,\n",
       " '16231': 501,\n",
       " '62010': 502,\n",
       " '41112': 503,\n",
       " '25114': 504,\n",
       " '37012': 505,\n",
       " '32021': 506,\n",
       " '23329': 507,\n",
       " '58222': 508,\n",
       " '41121': 509,\n",
       " '14491': 510,\n",
       " '16229': 511,\n",
       " '33999': 512,\n",
       " '30202': 513,\n",
       " '14191': 514,\n",
       " '01420': 515,\n",
       " '58122': 516,\n",
       " '61299': 517,\n",
       " '38130': 518,\n",
       " '14300': 519,\n",
       " '58111': 520,\n",
       " '01159': 521,\n",
       " '58211': 522,\n",
       " '58123': 523,\n",
       " '60210': 524,\n",
       " '59113': 525,\n",
       " '26212': 526,\n",
       " '24229': 527,\n",
       " '35130': 528,\n",
       " '29222': 529,\n",
       " 'ICT교육': 530,\n",
       " 'ICT음악': 531,\n",
       " '59142': 532,\n",
       " '02040': 533,\n",
       " '58112': 534,\n",
       " '61291': 535,\n",
       " 'ICT영상': 536,\n",
       " '37011': 537,\n",
       " 'ICT출판': 538,\n",
       " '26511': 539,\n",
       " '59130': 540,\n",
       " '14130': 541,\n",
       " '07110': 542,\n",
       " '27301': 543,\n",
       " '02030': 544,\n",
       " '26519': 545,\n",
       " '26323': 546,\n",
       " '26121': 547,\n",
       " '26529': 548,\n",
       " '28520': 549,\n",
       " '25921': 550,\n",
       " '27215': 551,\n",
       " '24133': 552,\n",
       " '60221': 553,\n",
       " '26299': 554,\n",
       " '28519': 555,\n",
       " '28511': 556,\n",
       " '29271': 557,\n",
       " '27212': 558,\n",
       " '26211': 559,\n",
       " '26422': 560,\n",
       " '27112': 561,\n",
       " '24329': 562,\n",
       " '26129': 563,\n",
       " '28202': 564,\n",
       " '26329': 565,\n",
       " '59202': 566}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksic_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '20411',\n",
       " 1: '25121',\n",
       " 2: '20491',\n",
       " 3: '30110',\n",
       " 4: '11111',\n",
       " 5: '23231',\n",
       " 6: '21210',\n",
       " 7: '29210',\n",
       " 8: '19102',\n",
       " 9: '14192',\n",
       " 10: '29193',\n",
       " 11: '24111',\n",
       " 12: '20132',\n",
       " 13: '20493',\n",
       " 14: '15211',\n",
       " 15: '29180',\n",
       " 16: '20311',\n",
       " 17: '27216',\n",
       " 18: '27192',\n",
       " 19: '38312',\n",
       " 20: '13401',\n",
       " 21: '27199',\n",
       " 22: '42491',\n",
       " 23: '10401',\n",
       " 24: '24211',\n",
       " 25: '17110',\n",
       " 26: '33920',\n",
       " 27: '10220',\n",
       " 28: '20321',\n",
       " 29: '10802',\n",
       " 30: '10794',\n",
       " 31: '20111',\n",
       " 32: '22199',\n",
       " 33: '29242',\n",
       " 34: '23232',\n",
       " 35: '28111',\n",
       " 36: '13101',\n",
       " 37: '29150',\n",
       " 38: '29194',\n",
       " 39: '23311',\n",
       " 40: '18121',\n",
       " 41: '10742',\n",
       " 42: '24113',\n",
       " 43: '12000',\n",
       " 44: '10713',\n",
       " 45: '19221',\n",
       " 46: '13211',\n",
       " 47: '24131',\n",
       " 48: '33992',\n",
       " 49: '26521',\n",
       " 50: '20499',\n",
       " 51: '29191',\n",
       " 52: '27302',\n",
       " 53: '19229',\n",
       " 54: '15219',\n",
       " 55: '24219',\n",
       " 56: '25119',\n",
       " 57: '16102',\n",
       " 58: '10792',\n",
       " 59: '24213',\n",
       " 60: '10743',\n",
       " 61: '27219',\n",
       " 62: '35200',\n",
       " 63: '20494',\n",
       " 64: '10403',\n",
       " 65: '29175',\n",
       " 66: '20121',\n",
       " 67: '28121',\n",
       " 68: '10799',\n",
       " 69: '28902',\n",
       " 70: '23119',\n",
       " 71: '29291',\n",
       " 72: '29141',\n",
       " 73: '25922',\n",
       " 74: '24191',\n",
       " 75: '10212',\n",
       " 76: '33991',\n",
       " 77: '24212',\n",
       " 78: '21220',\n",
       " 79: '28119',\n",
       " 80: '20112',\n",
       " 81: '33303',\n",
       " 82: '25923',\n",
       " 83: '33209',\n",
       " 84: '20129',\n",
       " 85: '10712',\n",
       " 86: '29293',\n",
       " 87: '20501',\n",
       " 88: '33110',\n",
       " 89: '20202',\n",
       " 90: '10751',\n",
       " 91: '29299',\n",
       " 92: '33932',\n",
       " 93: '10730',\n",
       " 94: '29176',\n",
       " 95: '19210',\n",
       " 96: '22229',\n",
       " 97: '10611',\n",
       " 98: '23211',\n",
       " 99: '15110',\n",
       " 100: '35112',\n",
       " 101: '11201',\n",
       " 102: '28201',\n",
       " 103: '10795',\n",
       " 104: '10309',\n",
       " 105: '25995',\n",
       " 106: '21300',\n",
       " 107: '23325',\n",
       " 108: '27400',\n",
       " 109: '22214',\n",
       " 110: '41223',\n",
       " 111: '29230',\n",
       " 112: '10502',\n",
       " 113: '22291',\n",
       " 114: '25931',\n",
       " 115: '10711',\n",
       " 116: '10801',\n",
       " 117: '20131',\n",
       " 118: '01123',\n",
       " 119: '33201',\n",
       " 120: '20423',\n",
       " 121: '22193',\n",
       " 122: '10211',\n",
       " 123: '31113',\n",
       " 124: '23111',\n",
       " 125: '10612',\n",
       " 126: '16211',\n",
       " 127: '20413',\n",
       " 128: '41222',\n",
       " 129: '35120',\n",
       " 130: '10129',\n",
       " 131: '29223',\n",
       " 132: '16103',\n",
       " 133: '29241',\n",
       " 134: '21230',\n",
       " 135: '03211',\n",
       " 136: '24290',\n",
       " 137: '23994',\n",
       " 138: '10501',\n",
       " 139: '29131',\n",
       " 140: '41224',\n",
       " 141: '10793',\n",
       " 142: '24222',\n",
       " 143: '10301',\n",
       " 144: '16212',\n",
       " 145: '27194',\n",
       " 146: '24112',\n",
       " 147: '29174',\n",
       " 148: '29172',\n",
       " 149: '29192',\n",
       " 150: '10402',\n",
       " 151: '42135',\n",
       " 152: '29294',\n",
       " 153: '22221',\n",
       " 154: '35113',\n",
       " 155: '01140',\n",
       " 156: '23919',\n",
       " 157: '22211',\n",
       " 158: '23999',\n",
       " 159: '26410',\n",
       " 160: '25942',\n",
       " 161: '22299',\n",
       " 162: '25130',\n",
       " 163: '26600',\n",
       " 164: '29111',\n",
       " 165: '24119',\n",
       " 166: '01151',\n",
       " 167: '20422',\n",
       " 168: '20492',\n",
       " 169: '29292',\n",
       " 170: '25914',\n",
       " 171: '28112',\n",
       " 172: '24221',\n",
       " 173: '28512',\n",
       " 174: '33401',\n",
       " 175: '22212',\n",
       " 176: '29261',\n",
       " 177: '01121',\n",
       " 178: '29269',\n",
       " 179: '22192',\n",
       " 180: '33910',\n",
       " 181: '28903',\n",
       " 182: '26294',\n",
       " 183: '26112',\n",
       " 184: '26221',\n",
       " 185: '23121',\n",
       " 186: '28423',\n",
       " 187: '33301',\n",
       " 188: '29163',\n",
       " 189: '35300',\n",
       " 190: '28909',\n",
       " 191: '25992',\n",
       " 192: '20201',\n",
       " 193: '26292',\n",
       " 194: '24132',\n",
       " 195: '22232',\n",
       " 196: '38220',\n",
       " 197: '20119',\n",
       " 198: '23222',\n",
       " 199: '28410',\n",
       " 200: '08000',\n",
       " 201: '30392',\n",
       " 202: '41111',\n",
       " 203: '42201',\n",
       " 204: '26111',\n",
       " 205: '10796',\n",
       " 206: '29250',\n",
       " 207: '01220',\n",
       " 208: '10213',\n",
       " 209: '23191',\n",
       " 210: '10122',\n",
       " 211: '10121',\n",
       " 212: '28421',\n",
       " 213: '29221',\n",
       " 214: '23129',\n",
       " 215: '33933',\n",
       " 216: '23324',\n",
       " 217: '28302',\n",
       " 218: '27191',\n",
       " 219: '28901',\n",
       " 220: '29162',\n",
       " 221: '02011',\n",
       " 222: '25911',\n",
       " 223: '27309',\n",
       " 224: '30320',\n",
       " 225: '25934',\n",
       " 226: '42203',\n",
       " 227: '21102',\n",
       " 228: '14111',\n",
       " 229: '26291',\n",
       " 230: '07210',\n",
       " 231: '42136',\n",
       " 232: '22223',\n",
       " 233: '25991',\n",
       " 234: '33931',\n",
       " 235: '29169',\n",
       " 236: '26421',\n",
       " 237: '29199',\n",
       " 238: '14419',\n",
       " 239: '42204',\n",
       " 240: '27214',\n",
       " 241: '24121',\n",
       " 242: '59201',\n",
       " 243: '23993',\n",
       " 244: '24312',\n",
       " 245: '28303',\n",
       " 246: '24322',\n",
       " 247: '24122',\n",
       " 248: '01110',\n",
       " 249: '23192',\n",
       " 250: '10613',\n",
       " 251: '38210',\n",
       " 252: '10219',\n",
       " 253: '24123',\n",
       " 254: '33409',\n",
       " 255: '29173',\n",
       " 256: '25912',\n",
       " 257: '32091',\n",
       " 258: '27213',\n",
       " 259: '29142',\n",
       " 260: '20203',\n",
       " 261: '25929',\n",
       " 262: '25943',\n",
       " 263: '22111',\n",
       " 264: '42123',\n",
       " 265: '01231',\n",
       " 266: '26295',\n",
       " 267: '24311',\n",
       " 268: '30122',\n",
       " 269: '42500',\n",
       " 270: '10620',\n",
       " 271: '33309',\n",
       " 272: '14199',\n",
       " 273: '30310',\n",
       " 274: '31202',\n",
       " 275: '29224',\n",
       " 276: '25932',\n",
       " 277: '31991',\n",
       " 278: '01299',\n",
       " 279: '25993',\n",
       " 280: '22292',\n",
       " 281: '32019',\n",
       " 282: '10749',\n",
       " 283: '41210',\n",
       " 284: '10720',\n",
       " 285: '35114',\n",
       " 286: '10791',\n",
       " 287: '31114',\n",
       " 288: '14200',\n",
       " 289: '23212',\n",
       " 290: '35111',\n",
       " 291: '33993',\n",
       " 292: '03112',\n",
       " 293: '19101',\n",
       " 294: '13910',\n",
       " 295: '39009',\n",
       " 296: '10741',\n",
       " 297: '28122',\n",
       " 298: '30201',\n",
       " 299: '26429',\n",
       " 300: '30331',\n",
       " 301: '01131',\n",
       " 302: '14120',\n",
       " 303: '26224',\n",
       " 304: '25924',\n",
       " 305: '24321',\n",
       " 306: '31111',\n",
       " 307: '30203',\n",
       " 308: '31311',\n",
       " 309: '29280',\n",
       " 310: '29119',\n",
       " 311: '25200',\n",
       " 312: '23312',\n",
       " 313: '21101',\n",
       " 314: '24199',\n",
       " 315: '29132',\n",
       " 316: '33202',\n",
       " 317: '22249',\n",
       " 318: '25944',\n",
       " 319: '10111',\n",
       " 320: '32029',\n",
       " 321: '23992',\n",
       " 322: '20421',\n",
       " 323: '29133',\n",
       " 324: '14112',\n",
       " 325: '15220',\n",
       " 326: '22241',\n",
       " 327: '28123',\n",
       " 328: '25123',\n",
       " 329: '23323',\n",
       " 330: '28113',\n",
       " 331: '29120',\n",
       " 332: '13221',\n",
       " 333: '35119',\n",
       " 334: '31322',\n",
       " 335: '32011',\n",
       " 336: '31201',\n",
       " 337: '20424',\n",
       " 338: '30391',\n",
       " 339: '22231',\n",
       " 340: '27193',\n",
       " 341: '23112',\n",
       " 342: '38240',\n",
       " 343: '27111',\n",
       " 344: '42137',\n",
       " 345: '13300',\n",
       " 346: '28429',\n",
       " 347: '22112',\n",
       " 348: '17211',\n",
       " 349: '23221',\n",
       " 350: '30121',\n",
       " 351: '42134',\n",
       " 352: '22222',\n",
       " 353: '42311',\n",
       " 354: '42420',\n",
       " 355: '59111',\n",
       " 356: '22191',\n",
       " 357: '29161',\n",
       " 358: '25122',\n",
       " 359: '60100',\n",
       " 360: '41225',\n",
       " 361: '22259',\n",
       " 362: '20502',\n",
       " 363: '41122',\n",
       " 364: '31920',\n",
       " 365: '23122',\n",
       " 366: '26321',\n",
       " 367: '25112',\n",
       " 368: '30332',\n",
       " 369: '30393',\n",
       " 370: '42138',\n",
       " 371: '42411',\n",
       " 372: '01291',\n",
       " 373: '42121',\n",
       " 374: '42131',\n",
       " 375: '10619',\n",
       " 376: '29229',\n",
       " 377: '16300',\n",
       " 378: '10759',\n",
       " 379: '31999',\n",
       " 380: '42110',\n",
       " 381: '25941',\n",
       " 382: '01211',\n",
       " 383: '14194',\n",
       " 384: '03213',\n",
       " 385: '30399',\n",
       " 386: '25999',\n",
       " 387: '23991',\n",
       " 388: '26222',\n",
       " 389: '42122',\n",
       " 390: '14193',\n",
       " 391: '28301',\n",
       " 392: '26293',\n",
       " 393: '38321',\n",
       " 394: '23995',\n",
       " 395: '25994',\n",
       " 396: '20495',\n",
       " 397: '42129',\n",
       " 398: '42412',\n",
       " 399: '26310',\n",
       " 400: '41221',\n",
       " 401: '58219',\n",
       " 402: '33402',\n",
       " 403: '31120',\n",
       " 404: '28422',\n",
       " 405: '61210',\n",
       " 406: '18111',\n",
       " 407: '15129',\n",
       " 408: '14411',\n",
       " 409: '42132',\n",
       " 410: '03120',\n",
       " 411: '32099',\n",
       " 412: '10797',\n",
       " 413: '31910',\n",
       " 414: '31112',\n",
       " 415: '42139',\n",
       " 416: '14499',\n",
       " 417: '38230',\n",
       " 418: '62021',\n",
       " 419: '60222',\n",
       " 420: '02020',\n",
       " 421: '22251',\n",
       " 422: '25933',\n",
       " 423: '29171',\n",
       " 424: '38322',\n",
       " 425: '33302',\n",
       " 426: '16299',\n",
       " 427: '41129',\n",
       " 428: '10302',\n",
       " 429: '23199',\n",
       " 430: '01411',\n",
       " 431: '25913',\n",
       " 432: '03220',\n",
       " 433: '26223',\n",
       " 434: '58113',\n",
       " 435: '23911',\n",
       " 436: '22213',\n",
       " 437: '16291',\n",
       " 438: '25111',\n",
       " 439: '42312',\n",
       " 440: '01122',\n",
       " 441: '59120',\n",
       " 442: '01152',\n",
       " 443: '16232',\n",
       " 444: '03111',\n",
       " 445: '27211',\n",
       " 446: '37022',\n",
       " 447: '31321',\n",
       " 448: '02012',\n",
       " 449: '20412',\n",
       " 450: '16101',\n",
       " 451: '26219',\n",
       " 452: '33120',\n",
       " 453: '58190',\n",
       " 454: '41119',\n",
       " 455: '37021',\n",
       " 456: '36020',\n",
       " 457: '42202',\n",
       " 458: '31312',\n",
       " 459: '15121',\n",
       " 460: '01412',\n",
       " 461: '62022',\n",
       " 462: '01239',\n",
       " 463: '38110',\n",
       " 464: '16292',\n",
       " 465: '26322',\n",
       " 466: '62090',\n",
       " 467: '42322',\n",
       " 468: '41226',\n",
       " 469: '03212',\n",
       " 470: '25113',\n",
       " 471: '60229',\n",
       " 472: '58212',\n",
       " 473: '42499',\n",
       " 474: '15190',\n",
       " 475: '23321',\n",
       " 476: '16221',\n",
       " 477: '59141',\n",
       " 478: '01132',\n",
       " 479: '29272',\n",
       " 480: '61220',\n",
       " 481: '23229',\n",
       " 482: '23322',\n",
       " 483: '01300',\n",
       " 484: '42321',\n",
       " 485: '42133',\n",
       " 486: '58221',\n",
       " 487: '42492',\n",
       " 488: '39001',\n",
       " 489: '01212',\n",
       " 490: '23239',\n",
       " 491: '28114',\n",
       " 492: '38120',\n",
       " 493: '58121',\n",
       " 494: '59112',\n",
       " 495: '10112',\n",
       " 496: '59114',\n",
       " 497: '42209',\n",
       " 498: '41229',\n",
       " 499: '38311',\n",
       " 500: '36010',\n",
       " 501: '16231',\n",
       " 502: '62010',\n",
       " 503: '41112',\n",
       " 504: '25114',\n",
       " 505: '37012',\n",
       " 506: '32021',\n",
       " 507: '23329',\n",
       " 508: '58222',\n",
       " 509: '41121',\n",
       " 510: '14491',\n",
       " 511: '16229',\n",
       " 512: '33999',\n",
       " 513: '30202',\n",
       " 514: '14191',\n",
       " 515: '01420',\n",
       " 516: '58122',\n",
       " 517: '61299',\n",
       " 518: '38130',\n",
       " 519: '14300',\n",
       " 520: '58111',\n",
       " 521: '01159',\n",
       " 522: '58211',\n",
       " 523: '58123',\n",
       " 524: '60210',\n",
       " 525: '59113',\n",
       " 526: '26212',\n",
       " 527: '24229',\n",
       " 528: '35130',\n",
       " 529: '29222',\n",
       " 530: 'ICT교육',\n",
       " 531: 'ICT음악',\n",
       " 532: '59142',\n",
       " 533: '02040',\n",
       " 534: '58112',\n",
       " 535: '61291',\n",
       " 536: 'ICT영상',\n",
       " 537: '37011',\n",
       " 538: 'ICT출판',\n",
       " 539: '26511',\n",
       " 540: '59130',\n",
       " 541: '14130',\n",
       " 542: '07110',\n",
       " 543: '27301',\n",
       " 544: '02030',\n",
       " 545: '26519',\n",
       " 546: '26323',\n",
       " 547: '26121',\n",
       " 548: '26529',\n",
       " 549: '28520',\n",
       " 550: '25921',\n",
       " 551: '27215',\n",
       " 552: '24133',\n",
       " 553: '60221',\n",
       " 554: '26299',\n",
       " 555: '28519',\n",
       " 556: '28511',\n",
       " 557: '29271',\n",
       " 558: '27212',\n",
       " 559: '26211',\n",
       " 560: '26422',\n",
       " 561: '27112',\n",
       " 562: '24329',\n",
       " 563: '26129',\n",
       " 564: '28202',\n",
       " 565: '26329',\n",
       " 566: '59202'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksic_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "# reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "# original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "reloaded_results = reloaded_model(tf.constant(examples))\n",
    "result = classifier_model.predict(tf.constant(examples))\n",
    "result_predicted = np.argmax(result, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565 26329\n",
      "565 26329\n",
      "565 26329\n",
      "181 28903\n",
      "563 26129\n",
      "561 27112\n"
     ]
    }
   ],
   "source": [
    "for i in result_predicted:\n",
    "    print(i, ksic_index_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 567)\n",
      "tf.Tensor(\n",
      "[[1.9431334e-03 4.0589331e-04 1.2175862e-03 ... 1.4876358e-03\n",
      "  2.8474210e-02 5.4941761e-08]\n",
      " [2.0188200e-03 3.3095386e-04 1.7554187e-03 ... 2.4041440e-03\n",
      "  4.7869626e-02 6.2883949e-08]\n",
      " [1.3102700e-03 2.7627705e-04 1.5226485e-03 ... 3.7246358e-03\n",
      "  6.5068580e-02 7.0045452e-08]\n",
      " [4.6185846e-03 3.1183194e-04 1.9605451e-03 ... 4.0138406e-03\n",
      "  9.9091623e-03 7.6360955e-08]\n",
      " [2.3445520e-03 1.8015013e-04 2.6380799e-03 ... 7.0497897e-03\n",
      "  5.1095746e-02 8.5871271e-08]\n",
      " [3.2486115e-03 3.5447354e-04 1.8490922e-03 ... 3.0158395e-03\n",
      "  2.7696563e-02 6.5916439e-08]], shape=(6, 567), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(reloaded_results.shape)\n",
    "print(tf.nn.softmax(reloaded_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "VBWzH6exlCPS",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from the model in memory:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "print_my_examples() missing 1 required positional argument: 'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults from the model in memory:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print_my_examples(examples, original_results)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mprint_my_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreloaded_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: print_my_examples() missing 1 required positional argument: 'results'"
     ]
    }
   ],
   "source": [
    "# print('Results from the saved model:')\n",
    "# print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "# print_my_examples(examples, original_results)\n",
    "print_my_examples(examples, reloaded_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cOmih754Y_M"
   },
   "source": [
    "If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "0FdVD3973S-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 본 발명은 OMA BCAST 서비스에서, 다양한 포맷으로 제작된 콘테츠들에 대해 각 가입자 단말기에서 별도의 변환 프로세싱 없이 콘텐츠를 재생할 수 있도록 서비스 제공자 측에서 각 가입자의 단말기에 적합하게 트랜스코딩하여 전송하는 방법에 관한 것이다. : score: 0.001466\n",
      "input: 마스터 패킷 제어기와 데이터 서비스 노드간의 Ｒ-Ｐ 연결실패시 브로드캐스트/멀티캐스트 서비스 제공 방법 및시스템 : score: 0.000067\n",
      "input: 모듈식 MBMS(MULTIMEDIA BROADCAST AND MULTICAST SERVICE) 전달을 위한 기술들 : score: 0.000046\n",
      "input: 통신 단말기를 이용한 방송관련 채팅 서비스 방법 및시스템 : score: 0.000090\n",
      "input: 상황 기반 비정상 모니터링을 위한 방법들 및 시스템들  : score: 0.000012\n",
      "input: 사용자의 프로파일 정보를 이용한 광고 시청을 통한 무료통화 서비스 제공 시스템 및 그 방법 : score: 0.000153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "serving_results = reloaded_model \\\n",
    "            .signatures['serving_default'](tf.constant(examples))\n",
    "\n",
    "serving_results = tf.sigmoid(serving_results['classifier'])\n",
    "\n",
    "print_my_examples(examples, serving_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocess_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    return encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = []\n",
    "preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1줄 단위 토크나이징\n",
    "for text in train_ds['text'][:2]:\n",
    "    print([text])\n",
    "    bert_raw_result = preprocess_model([text])\n",
    "    preprocessed_text.append(bert_raw_result)\n",
    "# print(tf.sigmoid(bert_raw_result))\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 단위 토크나이징\n",
    "text_preprocessed = preprocess_model(train_ds['text'][:10])\n",
    "print(text_preprocessed)\n",
    "# print(text_preprocessed['input_word_ids'])\n",
    "# input_word_ids, input_type_ids, input_mask\n",
    "# for i, item in enumerate(text_preprocessed['input_word_ids']):\n",
    "#     print(i, item[:10])\n",
    "# for i, item in enumerate(text_preprocessed['input_type_ids']):\n",
    "#     print(i, item[:10])\n",
    "for i, item in enumerate(text_preprocessed['input_mask']):\n",
    "    print(i, item[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 단위 토크나이징\n",
    "train_preprocessed_text = preprocess_model(train_ds['text'][:10])\n",
    "print(train_preprocessed_text)\n",
    "# print(text_preprocessed['input_word_ids'])\n",
    "# input_word_ids, input_type_ids, input_mask\n",
    "# for i, item in enumerate(text_preprocessed['input_word_ids']):\n",
    "#     print(i, item[:10])\n",
    "# for i, item in enumerate(text_preprocessed['input_type_ids']):\n",
    "#     print(i, item[:10])\n",
    "for i, item in enumerate(train_preprocessed_text['input_mask']):\n",
    "    print(i, item[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_preprocessed_text, train_ds['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_text = preprocessed_model()\n",
    "for text in train_ds['text'][:10]:\n",
    "    print(text)\n",
    "    bert_raw_result = preprocess_model(text)\n",
    "# print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "\n",
    "# text_test = train_input[40]\n",
    "# text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "# print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "# print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "# print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "# print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "# print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장한 csv 파일로부터 tf.data 를 사용하여 데이터 입력 파이프라인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tf.data를 사용하여 dataset 객체 생성\n",
    "# test_ds = tf.data.experimental.make_csv_dataset(['ksic/train_ds.csv'], batch_size=batch_size, label_name='label',\n",
    "#                                                num_epochs=3)\n",
    "# # train_ds = tf.data.TextLineDataset(['ksic/train_ds.csv'])\n",
    "# # test_ds = tf.data.TextLineDataset(['ksic/test_ds.csv'])\n",
    "# for batch, label in test_ds.take(1):\n",
    "#     for key, value in batch.items():\n",
    "#         print(key, value.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.data.experimental.CsvDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame 객체로부터 tf.data를 사용하여 데이터 입력 파이프라인 생성\n",
    "* tf.data.Dataset를 사용하여 데이터 로드하기\n",
    "* tf.data.Dataset.from_tensor_slices를 사용하여 pandas 데이터 프레임에서 값을 읽습니다.\n",
    "* tf.data.Dataset를 사용할 때의 이점 중 하나는 간단하고 효율적인 데이터 파이프라인을 작성할 수 있다는 것입니다.\n",
    "* 자세한 내용은 데이터 로드 가이드를 참조하세요.\n",
    "    * https://www.tensorflow.org/tutorials/load_data/pandas_dataframe\n",
    "    * https://www.tensorflow.org/guide/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_ds['text'], test_ds['label']))\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices(dict(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트가 unicode 문자열로 표현됨, 원인과 캐릭터셋 표현 방법 확인 필요\n",
    "* https://www.tensorflow.org/text/guide/unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(iter(test_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in test_dataset.take(2):\n",
    "#     print([item.numpy() for item in line])\n",
    "#     print(line)\n",
    "#     print(tf.strings.unicode_decode(line[0], input_encoding='UTF-8'))\n",
    "    tf.strings.unicode_encode(tf.RaggedTensor.from_tensor(line[0]), output_encoding='UTF-8')  # 잘 안되네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_ds['text'], train_ds['label']))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_ds['text'], test_ds['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_dataset.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for element in input_text.as_numpy_iterator():\n",
    "#     print(element)\n",
    "\n",
    "# dataset shuffle\n",
    "counter = tf.data.experimental.Counter()\n",
    "train_dataset = tf.data.Dataset.zip((counter, train_dataset))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=100)\n",
    "train_dataset = train_dataset.batch(20)\n",
    "\n",
    "# 데이터 확인\n",
    "# n,line_batch = next(iter(dataset))\n",
    "# print(n.numpy(), line_batch)\n",
    "# for text, label in input_text.take(batch_size):\n",
    "#     print(text.numpy(), label.numpy(), ksic_index_dict[label.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_ds.take(1).numpy())\n",
    "for item in train_dataset.take(batch_size):\n",
    "    print(item)\n",
    "#     label = label_batch.numpy()\n",
    "#     print(f'Label : {label} ({ksic_index_dict[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training\n",
    "* 여러 가지 방법으로 tf.keras 모델에 데이터를 주입할 수 있습니다. 파이썬 제너레이터(generator)와 넘파이 배열을 입력으로 사용할 수 있습니다.\n",
    "* tf.data 패키지를 사용하여 모델에 데이터를 주입하는 것이 권장되는 방법입니다. 이 패키지는 데이터 조작을 위한 고성능 클래스들을 포함하고 있습니다.\n",
    "* tf.queue는 데이터 구조로만 지원되고 입력 파이프라인으로는 지원되지 않습니다.\n",
    "\n",
    "#### 데이터셋 사용하기\n",
    "* 텐서플로 데이터셋(Datasets) 패키지(tfds)는 tf.data.Dataset 객체로 정의된 데이터셋을 적재하기 위한 유틸리티가 포함되어 있습니다.\n",
    "* 예를 들어 tfds를 사용하여 MNIST 데이터셋을 적재하는 코드는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfds.load(np.array(input_text))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_input['text'], train_input['label']))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_input['text'], test_input['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.cast(training_df[features].values, tf.float32),\n",
    "        tf.cast(training_df['target'].values, tf.int32)\n",
    "        \n",
    "# datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IwI_2bcIeX8"
   },
   "outputs": [],
   "source": [
    "# 오리지널 코드에서는 train과 test 데이터가 폴더를 사용하여 구분되어 있다. KSIC는 raw dataframe을 미리 잘라서 test를 생성해줘야 한다.\n",
    "# raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     'patent/train',\n",
    "#     batch_size=batch_size,\n",
    "#     validation_split=0.2,\n",
    "#     subset='training',\n",
    "#     seed=seed)\n",
    "\n",
    "# class_names = raw_train_ds.class_names\n",
    "# train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IwI_2bcIeX8"
   },
   "outputs": [],
   "source": [
    "# val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     'patent/train',\n",
    "#     batch_size=batch_size,\n",
    "#     validation_split=0.2,\n",
    "#     subset='validation',\n",
    "#     seed=seed)\n",
    "\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6IwI_2bcIeX8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     'patent/test',\n",
    "#     batch_size=batch_size)\n",
    "\n",
    "# test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGm10A5HRGXp"
   },
   "source": [
    "Let's take a look at a few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuxDkcvVIoev"
   },
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(batch_size):\n",
    "    print(text_batch.numpy())\n",
    "    label = label_batch.numpy()\n",
    "    print(f'Label : {label} ({ksic_index_dict[label]})')\n",
    "#     for i in range(3):\n",
    "#         print(f'test: {text_batch.numpy()[i][:100]}')\n",
    "#         label = label_batch.numpy()[i]\n",
    "#         print(f'Label : {label} ({class_names[label]})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "## The preprocessing model\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n",
    "\n",
    "The preprocessing model must be the one referenced by the documentation of the BERT model, which you can read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.\n",
    "\n",
    "Note: You will load the preprocessing model into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4naBiEE_cZX"
   },
   "source": [
    "Let's try the preprocessing model on some text and see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [],
   "source": [
    "text_test = train_input[40]\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqL7ihkN_862"
   },
   "source": [
    "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n",
    "\n",
    "Some other important points:\n",
    "- The input is truncated to 128 tokens. The number of tokens can be customized, and you can see more details on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
    "- The `input_type_ids` only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n",
    "\n",
    "Since this text preprocessor is a TensorFlow model, It can be included in your model directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "## Using the BERT model\n",
    "\n",
    "Before putting BERT into your own model, let's take a look at its outputs. You will load it from TF Hub and see the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXxYpK8ixL34"
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OoF9mebuSZc"
   },
   "outputs": [],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm61jDrezAll"
   },
   "source": [
    "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
    "\n",
    "- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire movie review.\n",
    "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token in the movie review.\n",
    "- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.\n",
    "\n",
    "For the fine-tuning you are going to use the `pooled_output` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## Define your model\n",
    "\n",
    "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n",
    "\n",
    "Note: for more information about the base model's input and output you can follow the model's URL for documentation. Here specifically, you don't need to worry about it because the preprocessing model will take care of that for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(len(ksic_label), activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs4yhFraBuGQ"
   },
   "source": [
    "Let's check that the model runs with the output of the preprocessing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTUzNV2JE2G3"
   },
   "source": [
    "The output is meaningless, of course, because the model has not been trained yet.\n",
    "\n",
    "Let's take a look at the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EmzyHZXKIpm"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "## Model training\n",
    "\n",
    "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), you'll use `losses.BinaryCrossentropy` loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWPOZE-L3AgE"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77psrpfzbxtp"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "For the learning rate (`init_lr`), you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.data.experimental.cardinality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### Loading the BERT model and training\n",
    "\n",
    "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpBuV5j2cS_b"
   },
   "source": [
    "Note: training time will vary depending on the complexity of the BERT model you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
    "# Epoch 1/5\n",
    "# 248/625 [==========>...................] - ETA: 8:30 - loss: 0.5818 - binary_accuracy: 0.6627\n",
    "# 04.29, libcudnn 없음 오류 발생, GPU 미사용시에 16core를 모두 활용 중이며, 1 epoch마다 13분 가량 소요\n",
    "\n",
    "# Next. multi_cased_L-12_H-768_A-12 사용\n",
    "# Next. KPC 분류 결과 활용\n",
    "\n",
    "# gpu 설정을 완료하니 1 epoch마다 1분으로 1/13 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.nvidia.com/rdp/cudnn-download\n",
    "\n",
    "# Download cuDNN v8.4.0 (April 1st, 2022), for CUDA 11.x\n",
    "#  - Local Installers for Windows and Linux, Ubuntu(x86_64, armsbsa)\n",
    "#   - Local Installer for Ubuntu20.04 x86_64 (Deb)\n",
    "\n",
    "# sudo dpkg -i libcudnn8-dev_8.1.1.33-1+cuda11.2_amd64.deb\n",
    "# [출처] [NVIDIA] Could not load dynamic library 'libcudnn.so.8' 에러 해결|작성자 뚝이파파\n",
    "# 의존성 에러 발생\n",
    "\n",
    "# linux용 파일을 다운로드받아 설치 시도.\n",
    "# 압축을 푼 후 필요한 파일들을 cuda toolkt 폴더로 복사.\n",
    "# $ sudo cp cuda/include/cudnn.h /usr/local/cuda/include\n",
    "# $ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\n",
    "# $ sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n",
    "# [출처] [NVIDIA] Could not load dynamic library 'libcudnn.so.8' 에러 해결|작성자 뚝이파파\n",
    "\n",
    "# sudo apt-get install libcupti-dev\n",
    "# sudo apt --fix-broken install\n",
    "\n",
    "# 이거 실행해보고 ubuntu에서 기록 남기자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "# history_dict = history.history\n",
    "# print(history_dict.keys())\n",
    "\n",
    "# acc = history_dict['binary_accuracy']\n",
    "# val_acc = history_dict['val_binary_accuracy']\n",
    "# loss = history_dict['loss']\n",
    "# val_loss = history_dict['val_loss']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "# fig = plt.figure(figsize=(10, 6))\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# # r is for \"solid red line\"\n",
    "# plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# # b is for \"solid blue line\"\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# # plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzJZCo-cf-Jf"
   },
   "source": [
    "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4gN1KwReLPN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "As a next step, you can try [Solve GLUE tasks using BERT on a TPU tutorial](https://www.tensorflow.org/text/tutorials/bert_glue), which runs on a TPU and shows you how to work with multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원시코드\n",
    "# https://github.com/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\n",
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/classify_text_with_bert\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/google/collections/bert/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A dependency of the preprocessing for BERT inputs\n",
    "# !pip install -q -U \"tensorflow-text==2.8.*\"\n",
    "# !pip install -q tf-models-official==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "google_bert2",
   "language": "python",
   "name": "google_bert_w_tutorial-master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
